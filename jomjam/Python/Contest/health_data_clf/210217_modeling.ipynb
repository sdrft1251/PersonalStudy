{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_labels.csv', 'test_features.csv', 'train_features.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir(\"/works/Data/Dacon/health_data_clf/\")\n",
    "file_list = [file for file in file_list if file.endswith(\".csv\")]\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3125, 3)\n",
      "(469200, 8)\n",
      "(1875000, 8)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/works/Data/Dacon/health_data_clf/\"\n",
    "train_label = pd.read_csv(file_path + file_list[0])\n",
    "test_features = pd.read_csv(file_path + file_list[1])\n",
    "train_features = pd.read_csv(file_path + file_list[2])\n",
    "sample_submissuib = pd.read_csv(file_path + file_list[3])\n",
    "\n",
    "print(train_label.shape)\n",
    "print(test_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index(arr, num):\n",
    "    return arr.argsort()[-num:]\n",
    "\n",
    "def prerpocessing_using_static(features_set, label_set):\n",
    "    input_set = []\n",
    "    target_set = []\n",
    "    \n",
    "    for id_idx in range(features_set.id.min(), features_set.id.max()+1):\n",
    "        sample_features = features_set[[\"acc_x\",\"acc_y\",\"acc_z\"]][features_set.id==id_idx].values\n",
    "        \n",
    "        #Fourier transform\n",
    "        strength = np.fft.fft(sample_features, axis=0)\n",
    "        strength = abs(strength)\n",
    "        frequency = np.fft.fftfreq(len(sample_features), 1)\n",
    "        strength_pos = strength[frequency>=0]\n",
    "        frequency_pos = frequency[frequency>=0]\n",
    "        freq_x = frequency_pos[return_index(strength_pos[:,0],3)].mean()\n",
    "        freq_y = frequency_pos[return_index(strength_pos[:,1],3)].mean()\n",
    "        freq_z = frequency_pos[return_index(strength_pos[:,2],3)].mean()\n",
    "        \n",
    "        #Transform to static\n",
    "        #Mean values\n",
    "        mean_arr = sample_features.mean(axis=0)\n",
    "        #Mag values\n",
    "        mag_arr = abs(sample_features).mean(axis=0)\n",
    "        #Std values\n",
    "        std_arr = sample_features.std(axis=0)\n",
    "        #Cov values\n",
    "        cov_xy = np.cov(sample_features[:,0], sample_features[:,1])[0][1]\n",
    "        cov_yz = np.cov(sample_features[:,1], sample_features[:,2])[0][1]\n",
    "        cov_zx = np.cov(sample_features[:,2], sample_features[:,0])[0][1]\n",
    "        #Corr values\n",
    "        cor_xy = cov_xy / (std_arr[0]*std_arr[1])\n",
    "        cor_yz = cov_yz / (std_arr[1]*std_arr[2])\n",
    "        cor_zx = cov_zx / (std_arr[2]*std_arr[0])\n",
    "        #Start End point change of vector values\n",
    "        start_point = sample_features[:3,:].mean(axis=0)\n",
    "        end_point = sample_features[-3:,:].mean(axis=0)\n",
    "        change_of_vector_st = np.cos((start_point * end_point).sum() / (np.sqrt(np.power(start_point, 2).sum()) * np.sqrt(np.power(end_point, 2).sum())))\n",
    "        #Energy values\n",
    "        shift_0_values = np.roll(sample_features, 1, axis=0)[1:]\n",
    "        shift_1_values = sample_features[1:]\n",
    "        energys = np.power((shift_0_values-shift_1_values),2)\n",
    "        energy_strength = np.fft.fft(energys, axis=0)\n",
    "        energy_strength = abs(energy_strength)\n",
    "        energy_frequency = np.fft.fftfreq(len(energy_strength), 1)\n",
    "        energy_strength_pos = energy_strength[energy_frequency>=0]\n",
    "        energy_frequency_pos = energy_frequency[energy_frequency>=0]\n",
    "        energy_freq_x = energy_frequency_pos[return_index(energy_strength_pos[:,0],3)].mean()\n",
    "        energy_freq_y = energy_frequency_pos[return_index(energy_strength_pos[:,1],3)].mean()\n",
    "        energy_freq_z = energy_frequency_pos[return_index(energy_strength_pos[:,2],3)].mean()\n",
    "        energy_mean = energys.mean(axis=0)\n",
    "        energy_std = energys.std(axis=0)\n",
    "        #Max Min point num\n",
    "        increase_shift_0 = np.where(shift_0_values-shift_1_values>=0, 1, -1)\n",
    "        increase_shift_1 = np.roll(increase_shift_0, 1, axis=0)\n",
    "        max_min_point_num = np.where(increase_shift_0[1:]*increase_shift_1[1:]==-1, 1, 0).sum(axis=0)\n",
    "        \n",
    "        #Make Set\n",
    "        freqs = [freq_x, freq_y, freq_z, energy_freq_x, energy_freq_y, energy_freq_z]\n",
    "        id_sample_set = list(mean_arr) + list(mag_arr) + list(std_arr)\\\n",
    "        + [cor_xy, cor_yz, cor_zx]\\\n",
    "        + [change_of_vector_st]\\\n",
    "        + list(energy_mean) + list(energy_std) + list(max_min_point_num) + freqs + [id_idx]\n",
    "        \n",
    "        #Append\n",
    "        input_set.append(id_sample_set)\n",
    "        target_set.append(label_set[label_set.id == id_idx].label.values[0])\n",
    "    \n",
    "    input_set = np.array(input_set)\n",
    "    target_set = np.array(target_set)\n",
    "    print(\"Input set : {} / Target set : {}\".format(input_set.shape, target_set.shape))\n",
    "    return input_set, target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : (3125, 29) / Target set : (3125,)\n"
     ]
    }
   ],
   "source": [
    "input_set, target_set = prerpocessing_using_static(features_set=train_features, label_set=train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812, 29) (2812,) (313, 29) (313,)\n",
      "(2812, 28) (2812,) (313, 28) (313,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(input_set, target_set, test_size=0.1, stratify=target_set)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "\n",
    "y_train_id = x_train[:,-1]\n",
    "x_train = x_train[:,:-1]\n",
    "\n",
    "y_val_id = x_val[:,-1]\n",
    "x_val = x_val[:,:-1]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6154\n",
      "[LightGBM] [Info] Number of data points in the train set: 2812, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -4.997212\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -3.475743\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -4.763597\n",
      "[LightGBM] [Info] Start training from score -4.203982\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.763597\n",
      "[LightGBM] [Info] Start training from score -5.108438\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -0.722009\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.049831\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.169063\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -4.645814\n",
      "[LightGBM] [Info] Start training from score -4.722775\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.997212\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.645814\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -5.456745\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -4.475915\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.180451\n",
      "[100]\tvalid_0's multi_logloss: 1.59849\n",
      "[200]\tvalid_0's multi_logloss: 1.25698\n",
      "[300]\tvalid_0's multi_logloss: 1.10746\n",
      "[400]\tvalid_0's multi_logloss: 0.943031\n",
      "[500]\tvalid_0's multi_logloss: 0.890014\n",
      "[600]\tvalid_0's multi_logloss: 0.84895\n",
      "[700]\tvalid_0's multi_logloss: 0.822841\n",
      "[800]\tvalid_0's multi_logloss: 0.807684\n",
      "[900]\tvalid_0's multi_logloss: 0.801311\n",
      "[1000]\tvalid_0's multi_logloss: 0.788235\n",
      "[1100]\tvalid_0's multi_logloss: 0.781199\n",
      "[1200]\tvalid_0's multi_logloss: 0.77694\n",
      "[1300]\tvalid_0's multi_logloss: 0.775493\n"
     ]
    }
   ],
   "source": [
    "train_ds = lgb.Dataset(x_train, label=y_train)\n",
    "val_ds = lgb.Dataset(x_val, label=y_val)\n",
    "params = {\n",
    "    'learning_rate' : 0.05,\n",
    "    'boosting_type' : 'dart',\n",
    "    'max_depth' : 3,\n",
    "    'num_leaves' : 2,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'drop_rate' : 0.6,\n",
    "    'feature_fraction' : 0.2,\n",
    "    'bagging_fraction' : 0.2,\n",
    "    'objective' : 'multiclass',\n",
    "    'metric' : 'multi_logloss',\n",
    "    'num_class':61\n",
    "}\n",
    "\n",
    "clf = lgb.train(params, train_ds, 1300, val_ds, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626600284495022\n",
      "0.7763578274760383\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(x_train)\n",
    "val_pred = clf.predict(x_val)\n",
    "\n",
    "def return_armax(arrs):\n",
    "    return np.argmax(arrs, axis=1)\n",
    "train_pred = return_armax(train_pred)\n",
    "val_pred = return_armax(val_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(accuracy_score(y_train, train_pred))\n",
    "print(accuracy_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prerpocessing_using_static_sub(features_set):\n",
    "    input_set = []\n",
    "    \n",
    "    for id_idx in range(features_set.id.min(), features_set.id.max()+1):\n",
    "        sample_features = features_set[[\"acc_x\",\"acc_y\",\"acc_z\"]][features_set.id==id_idx].values\n",
    "        \n",
    "        #Fourier transform\n",
    "        strength = np.fft.fft(sample_features, axis=0)\n",
    "        strength = abs(strength)\n",
    "        frequency = np.fft.fftfreq(len(sample_features), 1)\n",
    "        strength_pos = strength[frequency>=0]\n",
    "        frequency_pos = frequency[frequency>=0]\n",
    "        freq_x = frequency_pos[return_index(strength_pos[:,0],3)].mean()\n",
    "        freq_y = frequency_pos[return_index(strength_pos[:,1],3)].mean()\n",
    "        freq_z = frequency_pos[return_index(strength_pos[:,2],3)].mean()\n",
    "        \n",
    "        #Transform to static\n",
    "        #Mean values\n",
    "        mean_arr = sample_features.mean(axis=0)\n",
    "        #Mag values\n",
    "        mag_arr = abs(sample_features).mean(axis=0)\n",
    "        #Std values\n",
    "        std_arr = sample_features.std(axis=0)\n",
    "        #Cov values\n",
    "        cov_xy = np.cov(sample_features[:,0], sample_features[:,1])[0][1]\n",
    "        cov_yz = np.cov(sample_features[:,1], sample_features[:,2])[0][1]\n",
    "        cov_zx = np.cov(sample_features[:,2], sample_features[:,0])[0][1]\n",
    "        #Corr values\n",
    "        cor_xy = cov_xy / (std_arr[0]*std_arr[1])\n",
    "        cor_yz = cov_yz / (std_arr[1]*std_arr[2])\n",
    "        cor_zx = cov_zx / (std_arr[2]*std_arr[0])\n",
    "        #Start End point change of vector values\n",
    "        start_point = sample_features[:3,:].mean(axis=0)\n",
    "        end_point = sample_features[-3:,:].mean(axis=0)\n",
    "        change_of_vector_st = np.cos((start_point * end_point).sum() / (np.sqrt(np.power(start_point, 2).sum()) * np.sqrt(np.power(end_point, 2).sum())))\n",
    "        #Energy values\n",
    "        shift_0_values = np.roll(sample_features, 1, axis=0)[1:]\n",
    "        shift_1_values = sample_features[1:]\n",
    "        energys = np.power((shift_0_values-shift_1_values),2)\n",
    "        energy_strength = np.fft.fft(energys, axis=0)\n",
    "        energy_strength = abs(energy_strength)\n",
    "        energy_frequency = np.fft.fftfreq(len(energy_strength), 1)\n",
    "        energy_strength_pos = energy_strength[energy_frequency>=0]\n",
    "        energy_frequency_pos = energy_frequency[energy_frequency>=0]\n",
    "        energy_freq_x = energy_frequency_pos[return_index(energy_strength_pos[:,0],3)].mean()\n",
    "        energy_freq_y = energy_frequency_pos[return_index(energy_strength_pos[:,1],3)].mean()\n",
    "        energy_freq_z = energy_frequency_pos[return_index(energy_strength_pos[:,2],3)].mean()\n",
    "        energy_mean = energys.mean(axis=0)\n",
    "        energy_std = energys.std(axis=0)\n",
    "        #Max Min point num\n",
    "        increase_shift_0 = np.where(shift_0_values-shift_1_values>=0, 1, -1)\n",
    "        increase_shift_1 = np.roll(increase_shift_0, 1, axis=0)\n",
    "        max_min_point_num = np.where(increase_shift_0[1:]*increase_shift_1[1:]==-1, 1, 0).sum(axis=0)\n",
    "        \n",
    "        #Make Set\n",
    "        freqs = [freq_x, freq_y, freq_z, energy_freq_x, energy_freq_y, energy_freq_z]\n",
    "        id_sample_set = list(mean_arr) + list(mag_arr) + list(std_arr)\\\n",
    "        + [cor_xy, cor_yz, cor_zx]\\\n",
    "        + [change_of_vector_st]\\\n",
    "        + list(energy_mean) + list(energy_std) + list(max_min_point_num) + freqs\n",
    "        \n",
    "        #Append\n",
    "        input_set.append(id_sample_set)\n",
    "    \n",
    "    input_set = np.array(input_set)\n",
    "    print(\"Input set : {}\".format(input_set.shape))\n",
    "    return input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : (782, 28)\n",
      "(782, 28)\n"
     ]
    }
   ],
   "source": [
    "sub_input_set =  prerpocessing_using_static_sub(features_set=test_features)\n",
    "print(sub_input_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = clf.predict(sub_input_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['id'] = sample_submissuib.id\n",
    "for i in range(61):\n",
    "    result_df[i] = sub_pred[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3125</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>3.190235e-04</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>4.003686e-04</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>5.021264e-08</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3126</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>4.088824e-06</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>9.199024e-05</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>3.519291e-07</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3127</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.702971</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>4.897223e-07</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>7.430800e-04</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>4.176305e-05</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3128</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>4.505711e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2.093899e-07</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.364746e-07</td>\n",
       "      <td>0.040401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3129</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>2.044785e-05</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1.205354e-04</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1.434244e-06</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id         0         1         2         3             4         5  \\\n",
       "0  3125  0.001991  0.000067  0.000330  0.001266  3.190235e-04  0.000015   \n",
       "1  3126  0.003855  0.000056  0.000162  0.001198  4.088824e-06  0.000060   \n",
       "2  3127  0.002162  0.702971  0.000113  0.001469  4.897223e-07  0.000426   \n",
       "3  3128  0.000051  0.000036  0.000093  0.000007  4.505711e-06  0.000026   \n",
       "4  3129  0.000565  0.002646  0.000036  0.000266  2.044785e-05  0.000069   \n",
       "\n",
       "              6         7         8  ...        51        52        53  \\\n",
       "0  4.003686e-04  0.000029  0.000554  ...  0.009534  0.000946  0.000905   \n",
       "1  9.199024e-05  0.000766  0.000083  ...  0.000118  0.000153  0.000055   \n",
       "2  7.430800e-04  0.000222  0.002062  ...  0.000165  0.000138  0.000144   \n",
       "3  2.093899e-07  0.002410  0.000125  ...  0.000033  0.000007  0.000005   \n",
       "4  1.205354e-04  0.000654  0.000128  ...  0.000662  0.000029  0.000805   \n",
       "\n",
       "         54        55        56        57        58            59        60  \n",
       "0  0.000054  0.000468  0.000137  0.000020  0.007560  5.021264e-08  0.000122  \n",
       "1  0.000122  0.000007  0.000052  0.000006  0.000018  3.519291e-07  0.000380  \n",
       "2  0.007392  0.000258  0.002136  0.000008  0.000133  4.176305e-05  0.000066  \n",
       "3  0.001800  0.000010  0.000042  0.000013  0.000004  6.364746e-07  0.040401  \n",
       "4  0.000030  0.000004  0.000240  0.000335  0.000030  1.434244e-06  0.000364  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"/works/Data/Dacon/health_data_clf/result/210217.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
