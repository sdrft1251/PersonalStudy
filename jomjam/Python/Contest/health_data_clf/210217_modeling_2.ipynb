{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_labels.csv', 'test_features.csv', 'train_features.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir(\"/works/Data/Dacon/health_data_clf/\")\n",
    "file_list = [file for file in file_list if file.endswith(\".csv\")]\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3125, 3)\n",
      "(469200, 8)\n",
      "(1875000, 8)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/works/Data/Dacon/health_data_clf/\"\n",
    "train_label = pd.read_csv(file_path + file_list[0])\n",
    "test_features = pd.read_csv(file_path + file_list[1])\n",
    "train_features = pd.read_csv(file_path + file_list[2])\n",
    "sample_submissuib = pd.read_csv(file_path + file_list[3])\n",
    "\n",
    "print(train_label.shape)\n",
    "print(test_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index(arr, num):\n",
    "    return arr.argsort()[-num:]\n",
    "\n",
    "def prerpocessing_using_static(features_set, label_set):\n",
    "    input_set = []\n",
    "    target_set = []\n",
    "    \n",
    "    for id_idx in range(features_set.id.min(), features_set.id.max()+1):\n",
    "        sample_features = features_set[[\"acc_x\",\"acc_y\",\"acc_z\", \"gy_x\", \"gy_y\", \"gy_z\"]][features_set.id==id_idx].values\n",
    "        \n",
    "        #Fourier transform\n",
    "        strength = np.fft.fft(sample_features, axis=0)\n",
    "        strength = abs(strength)\n",
    "        frequency = np.fft.fftfreq(len(sample_features), 1)\n",
    "        strength_pos = strength[frequency>=0]\n",
    "        frequency_pos = frequency[frequency>=0]\n",
    "        freq_x = frequency_pos[return_index(strength_pos[:,0],3)].mean()\n",
    "        freq_y = frequency_pos[return_index(strength_pos[:,1],3)].mean()\n",
    "        freq_z = frequency_pos[return_index(strength_pos[:,2],3)].mean()\n",
    "        freq_gy_x = frequency_pos[return_index(strength_pos[:,3],3)].mean()\n",
    "        freq_gy_y = frequency_pos[return_index(strength_pos[:,4],3)].mean()\n",
    "        freq_gy_z = frequency_pos[return_index(strength_pos[:,5],3)].mean()\n",
    "        \n",
    "        #Transform to static\n",
    "        #Mean values\n",
    "        mean_arr = sample_features.mean(axis=0)\n",
    "        #Mag values\n",
    "        mag_arr = abs(sample_features).mean(axis=0)\n",
    "        #Std values\n",
    "        std_arr = sample_features.std(axis=0)\n",
    "        #Cov values\n",
    "        cov_xy = np.cov(sample_features[:,0], sample_features[:,1])[0][1]\n",
    "        cov_yz = np.cov(sample_features[:,1], sample_features[:,2])[0][1]\n",
    "        cov_zx = np.cov(sample_features[:,2], sample_features[:,0])[0][1]\n",
    "        cov_gy_xy = np.cov(sample_features[:,3], sample_features[:,4])[0][1]\n",
    "        cov_gy_yz = np.cov(sample_features[:,4], sample_features[:,5])[0][1]\n",
    "        cov_gy_zx = np.cov(sample_features[:,5], sample_features[:,3])[0][1]\n",
    "        #Corr values\n",
    "        cor_xy = cov_xy / (std_arr[0]*std_arr[1])\n",
    "        cor_yz = cov_yz / (std_arr[1]*std_arr[2])\n",
    "        cor_zx = cov_zx / (std_arr[2]*std_arr[0])\n",
    "        cor_gy_xy = cov_gy_xy / (std_arr[3]*std_arr[4])\n",
    "        cor_gy_yz = cov_gy_yz / (std_arr[4]*std_arr[5])\n",
    "        cor_gy_zx = cov_gy_zx / (std_arr[5]*std_arr[3])\n",
    "        #Start End point change of vector values\n",
    "        start_point = sample_features[:3,:3].mean(axis=0)\n",
    "        end_point = sample_features[-3:,:3].mean(axis=0)\n",
    "        change_of_vector_st = np.cos((start_point * end_point).sum() / (np.sqrt(np.power(start_point, 2).sum()) * np.sqrt(np.power(end_point, 2).sum())))\n",
    "        start_gy_point = sample_features[:3,3:].mean(axis=0)\n",
    "        end_gy_point = sample_features[-3:,3:].mean(axis=0)\n",
    "        change_of_vector_st_gy = np.cos((start_gy_point * end_gy_point).sum() / (np.sqrt(np.power(start_gy_point, 2).sum()) * np.sqrt(np.power(end_gy_point, 2).sum())))\n",
    "        \n",
    "        #Energy values\n",
    "        shift_0_values = np.roll(sample_features, 1, axis=0)[1:]\n",
    "        shift_1_values = sample_features[1:]\n",
    "        energys = np.power((shift_0_values-shift_1_values),2)\n",
    "        energy_strength = np.fft.fft(energys, axis=0)\n",
    "        energy_strength = abs(energy_strength)\n",
    "        energy_frequency = np.fft.fftfreq(len(energy_strength), 1)\n",
    "        energy_strength_pos = energy_strength[energy_frequency>=0]\n",
    "        energy_frequency_pos = energy_frequency[energy_frequency>=0]\n",
    "        energy_freq_x = energy_frequency_pos[return_index(energy_strength_pos[:,0],3)].mean()\n",
    "        energy_freq_y = energy_frequency_pos[return_index(energy_strength_pos[:,1],3)].mean()\n",
    "        energy_freq_z = energy_frequency_pos[return_index(energy_strength_pos[:,2],3)].mean()\n",
    "        energy_freq_gy_x = energy_frequency_pos[return_index(energy_strength_pos[:,3],3)].mean()\n",
    "        energy_freq_gy_y = energy_frequency_pos[return_index(energy_strength_pos[:,4],3)].mean()\n",
    "        energy_freq_gy_z = energy_frequency_pos[return_index(energy_strength_pos[:,5],3)].mean()\n",
    "        energy_mean = energys.mean(axis=0)\n",
    "        energy_std = energys.std(axis=0)\n",
    "        #Max Min point num\n",
    "        increase_shift_0 = np.where(shift_0_values-shift_1_values>=0, 1, -1)\n",
    "        increase_shift_1 = np.roll(increase_shift_0, 1, axis=0)\n",
    "        max_min_point_num = np.where(increase_shift_0[1:]*increase_shift_1[1:]==-1, 1, 0).sum(axis=0)\n",
    "        \n",
    "        #Make Set\n",
    "        id_sample_set = [mean_arr[0], mag_arr[0]] + list(mag_arr[3:]) + list(std_arr)\\\n",
    "        + [energy_mean[0], energy_mean[1], energy_std[0]]\\\n",
    "        + list(max_min_point_num) + [freq_gy_x, freq_gy_y, freq_gy_z]\n",
    "        \n",
    "        #Append\n",
    "        input_set.append(id_sample_set)\n",
    "        target_set.append(label_set[label_set.id == id_idx].label.values[0])\n",
    "    \n",
    "    input_set = np.array(input_set)\n",
    "    target_set = np.array(target_set)\n",
    "    print(\"Input set : {} / Target set : {}\".format(input_set.shape, target_set.shape))\n",
    "    return input_set, target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : (3125, 23) / Target set : (3125,)\n"
     ]
    }
   ],
   "source": [
    "input_set, target_set = prerpocessing_using_static(features_set=train_features, label_set=train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812, 23) (2812,) (313, 23) (313,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(input_set, target_set, test_size=0.1, stratify=target_set)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5389\n",
      "[LightGBM] [Info] Number of data points in the train set: 2812, number of used features: 23\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -4.997212\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -3.475743\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -4.763597\n",
      "[LightGBM] [Info] Start training from score -4.203982\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.763597\n",
      "[LightGBM] [Info] Start training from score -5.108438\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -0.722009\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.049831\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.169063\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -4.645814\n",
      "[LightGBM] [Info] Start training from score -4.722775\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.507664\n",
      "[LightGBM] [Info] Start training from score -4.997212\n",
      "[LightGBM] [Info] Start training from score -4.945919\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.806157\n",
      "[LightGBM] [Info] Start training from score -4.645814\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -4.850609\n",
      "[LightGBM] [Info] Start training from score -5.543756\n",
      "[LightGBM] [Info] Start training from score -5.456745\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.445144\n",
      "[LightGBM] [Info] Start training from score -4.475915\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -5.051279\n",
      "[LightGBM] [Info] Start training from score -4.897129\n",
      "[LightGBM] [Info] Start training from score -4.180451\n",
      "[100]\tvalid_0's multi_logloss: 1.64028\n",
      "[200]\tvalid_0's multi_logloss: 1.31671\n",
      "[300]\tvalid_0's multi_logloss: 1.1695\n",
      "[400]\tvalid_0's multi_logloss: 1.0047\n",
      "[500]\tvalid_0's multi_logloss: 0.959661\n",
      "[600]\tvalid_0's multi_logloss: 0.920096\n",
      "[700]\tvalid_0's multi_logloss: 0.888412\n",
      "[800]\tvalid_0's multi_logloss: 0.872605\n",
      "[900]\tvalid_0's multi_logloss: 0.866289\n",
      "[1000]\tvalid_0's multi_logloss: 0.853115\n",
      "[1100]\tvalid_0's multi_logloss: 0.845225\n",
      "[1200]\tvalid_0's multi_logloss: 0.837999\n",
      "[1300]\tvalid_0's multi_logloss: 0.833777\n"
     ]
    }
   ],
   "source": [
    "train_ds = lgb.Dataset(x_train, label=y_train)\n",
    "val_ds = lgb.Dataset(x_val, label=y_val)\n",
    "params = {\n",
    "    'learning_rate' : 0.05,\n",
    "    'boosting_type' : 'dart',\n",
    "    'max_depth' : 3,\n",
    "    'num_leaves' : 2,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'drop_rate' : 0.6,\n",
    "    'feature_fraction' : 0.2,\n",
    "    'bagging_fraction' : 0.2,\n",
    "    'objective' : 'multiclass',\n",
    "    'metric' : 'multi_logloss',\n",
    "    'num_class':61\n",
    "}\n",
    "\n",
    "clf = lgb.train(params, train_ds, 1300, val_ds, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9591038406827881\n",
      "0.7667731629392971\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(x_train)\n",
    "val_pred = clf.predict(x_val)\n",
    "\n",
    "def return_armax(arrs):\n",
    "    return np.argmax(arrs, axis=1)\n",
    "train_pred = return_armax(train_pred)\n",
    "val_pred = return_armax(val_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(accuracy_score(y_train, train_pred))\n",
    "print(accuracy_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index(arr, num):\n",
    "    return arr.argsort()[-num:]\n",
    "\n",
    "def prerpocessing_using_static_sub(features_set):\n",
    "    input_set = []\n",
    "\n",
    "    for id_idx in range(features_set.id.min(), features_set.id.max()+1):\n",
    "        sample_features = features_set[[\"acc_x\",\"acc_y\",\"acc_z\", \"gy_x\", \"gy_y\", \"gy_z\"]][features_set.id==id_idx].values\n",
    "        \n",
    "        #Fourier transform\n",
    "        strength = np.fft.fft(sample_features, axis=0)\n",
    "        strength = abs(strength)\n",
    "        frequency = np.fft.fftfreq(len(sample_features), 1)\n",
    "        strength_pos = strength[frequency>=0]\n",
    "        frequency_pos = frequency[frequency>=0]\n",
    "        freq_x = frequency_pos[return_index(strength_pos[:,0],3)].mean()\n",
    "        freq_y = frequency_pos[return_index(strength_pos[:,1],3)].mean()\n",
    "        freq_z = frequency_pos[return_index(strength_pos[:,2],3)].mean()\n",
    "        freq_gy_x = frequency_pos[return_index(strength_pos[:,3],3)].mean()\n",
    "        freq_gy_y = frequency_pos[return_index(strength_pos[:,4],3)].mean()\n",
    "        freq_gy_z = frequency_pos[return_index(strength_pos[:,5],3)].mean()\n",
    "        \n",
    "        #Transform to static\n",
    "        #Mean values\n",
    "        mean_arr = sample_features.mean(axis=0)\n",
    "        #Mag values\n",
    "        mag_arr = abs(sample_features).mean(axis=0)\n",
    "        #Std values\n",
    "        std_arr = sample_features.std(axis=0)\n",
    "        #Cov values\n",
    "        cov_xy = np.cov(sample_features[:,0], sample_features[:,1])[0][1]\n",
    "        cov_yz = np.cov(sample_features[:,1], sample_features[:,2])[0][1]\n",
    "        cov_zx = np.cov(sample_features[:,2], sample_features[:,0])[0][1]\n",
    "        cov_gy_xy = np.cov(sample_features[:,3], sample_features[:,4])[0][1]\n",
    "        cov_gy_yz = np.cov(sample_features[:,4], sample_features[:,5])[0][1]\n",
    "        cov_gy_zx = np.cov(sample_features[:,5], sample_features[:,3])[0][1]\n",
    "        #Corr values\n",
    "        cor_xy = cov_xy / (std_arr[0]*std_arr[1])\n",
    "        cor_yz = cov_yz / (std_arr[1]*std_arr[2])\n",
    "        cor_zx = cov_zx / (std_arr[2]*std_arr[0])\n",
    "        cor_gy_xy = cov_gy_xy / (std_arr[3]*std_arr[4])\n",
    "        cor_gy_yz = cov_gy_yz / (std_arr[4]*std_arr[5])\n",
    "        cor_gy_zx = cov_gy_zx / (std_arr[5]*std_arr[3])\n",
    "        #Start End point change of vector values\n",
    "        start_point = sample_features[:3,:3].mean(axis=0)\n",
    "        end_point = sample_features[-3:,:3].mean(axis=0)\n",
    "        change_of_vector_st = np.cos((start_point * end_point).sum() / (np.sqrt(np.power(start_point, 2).sum()) * np.sqrt(np.power(end_point, 2).sum())))\n",
    "        start_gy_point = sample_features[:3,3:].mean(axis=0)\n",
    "        end_gy_point = sample_features[-3:,3:].mean(axis=0)\n",
    "        change_of_vector_st_gy = np.cos((start_gy_point * end_gy_point).sum() / (np.sqrt(np.power(start_gy_point, 2).sum()) * np.sqrt(np.power(end_gy_point, 2).sum())))\n",
    "        \n",
    "        #Energy values\n",
    "        shift_0_values = np.roll(sample_features, 1, axis=0)[1:]\n",
    "        shift_1_values = sample_features[1:]\n",
    "        energys = np.power((shift_0_values-shift_1_values),2)\n",
    "        energy_strength = np.fft.fft(energys, axis=0)\n",
    "        energy_strength = abs(energy_strength)\n",
    "        energy_frequency = np.fft.fftfreq(len(energy_strength), 1)\n",
    "        energy_strength_pos = energy_strength[energy_frequency>=0]\n",
    "        energy_frequency_pos = energy_frequency[energy_frequency>=0]\n",
    "        energy_freq_x = energy_frequency_pos[return_index(energy_strength_pos[:,0],3)].mean()\n",
    "        energy_freq_y = energy_frequency_pos[return_index(energy_strength_pos[:,1],3)].mean()\n",
    "        energy_freq_z = energy_frequency_pos[return_index(energy_strength_pos[:,2],3)].mean()\n",
    "        energy_freq_gy_x = energy_frequency_pos[return_index(energy_strength_pos[:,3],3)].mean()\n",
    "        energy_freq_gy_y = energy_frequency_pos[return_index(energy_strength_pos[:,4],3)].mean()\n",
    "        energy_freq_gy_z = energy_frequency_pos[return_index(energy_strength_pos[:,5],3)].mean()\n",
    "        energy_mean = energys.mean(axis=0)\n",
    "        energy_std = energys.std(axis=0)\n",
    "        #Max Min point num\n",
    "        increase_shift_0 = np.where(shift_0_values-shift_1_values>=0, 1, -1)\n",
    "        increase_shift_1 = np.roll(increase_shift_0, 1, axis=0)\n",
    "        max_min_point_num = np.where(increase_shift_0[1:]*increase_shift_1[1:]==-1, 1, 0).sum(axis=0)\n",
    "        \n",
    "        #Make Set\n",
    "        id_sample_set = [mean_arr[0], mag_arr[0]] + list(mag_arr[3:]) + list(std_arr)\\\n",
    "        + [energy_mean[0], energy_mean[1], energy_std[0]]\\\n",
    "        + list(max_min_point_num) + [freq_gy_x, freq_gy_y, freq_gy_z]\n",
    "        \n",
    "        #Append\n",
    "        input_set.append(id_sample_set)\n",
    "    \n",
    "    input_set = np.array(input_set)\n",
    "    print(\"Input set : {}\".format(input_set.shape))\n",
    "    return input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : (782, 23)\n",
      "(782, 23)\n"
     ]
    }
   ],
   "source": [
    "sub_input_set =  prerpocessing_using_static_sub(features_set=test_features)\n",
    "print(sub_input_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = clf.predict(sub_input_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['id'] = sample_submissuib.id\n",
    "for i in range(61):\n",
    "    result_df[i] = sub_pred[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3125</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.030462e-04</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>8.685772e-04</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.078648e-03</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>2.589344e-04</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>1.669344e-07</td>\n",
       "      <td>0.002858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3126</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.455337e-05</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.547504e-08</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>7.468432e-06</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.748706e-05</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>3.445375e-08</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3127</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>2.298600e-04</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>1.974037e-07</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>1.788182e-04</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>3.792500e-04</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>2.465759e-04</td>\n",
       "      <td>0.000519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3128</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>7.537984e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.024373e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5.169889e-07</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>2.074373e-04</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.373644e-08</td>\n",
       "      <td>0.004036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3129</td>\n",
       "      <td>0.009225</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>8.527394e-07</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>7.772278e-05</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>6.012295e-05</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7.217129e-07</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>1.484582e-07</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id         0         1             2         3             4         5  \\\n",
       "0  3125  0.000885  0.000019  2.030462e-04  0.001398  8.685772e-04  0.000002   \n",
       "1  3126  0.000176  0.000019  1.455337e-05  0.000036  1.547504e-08  0.000113   \n",
       "2  3127  0.000062  0.037432  2.298600e-04  0.000699  1.974037e-07  0.000214   \n",
       "3  3128  0.000901  0.000042  7.537984e-06  0.000001  2.024373e-08  0.000007   \n",
       "4  3129  0.009225  0.001996  8.527394e-07  0.000166  7.772278e-05  0.000016   \n",
       "\n",
       "              6         7         8  ...        51        52        53  \\\n",
       "0  1.078648e-03  0.000007  0.000395  ...  0.098816  0.006190  0.002231   \n",
       "1  7.468432e-06  0.000037  0.000015  ...  0.000018  0.000012  0.000076   \n",
       "2  1.788182e-04  0.000191  0.000689  ...  0.002554  0.000021  0.000136   \n",
       "3  5.169889e-07  0.000052  0.000020  ...  0.000201  0.000016  0.000009   \n",
       "4  6.012295e-05  0.002719  0.000167  ...  0.000090  0.000068  0.000010   \n",
       "\n",
       "         54            55        56        57        58            59  \\\n",
       "0  0.000192  2.589344e-04  0.000009  0.000103  0.064930  1.669344e-07   \n",
       "1  0.000012  1.748706e-05  0.000021  0.000033  0.000044  3.445375e-08   \n",
       "2  0.641515  3.792500e-04  0.000491  0.000009  0.000329  2.465759e-04   \n",
       "3  0.000256  2.074373e-04  0.000115  0.000011  0.000003  3.373644e-08   \n",
       "4  0.000002  7.217129e-07  0.000018  0.000479  0.000219  1.484582e-07   \n",
       "\n",
       "         60  \n",
       "0  0.002858  \n",
       "1  0.000079  \n",
       "2  0.000519  \n",
       "3  0.004036  \n",
       "4  0.000633  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"/works/Data/Dacon/health_data_clf/result/210217_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
