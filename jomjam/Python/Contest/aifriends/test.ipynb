{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4752, 57)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X00</th>\n",
       "      <th>X01</th>\n",
       "      <th>X02</th>\n",
       "      <th>X03</th>\n",
       "      <th>X04</th>\n",
       "      <th>X05</th>\n",
       "      <th>X06</th>\n",
       "      <th>X07</th>\n",
       "      <th>X08</th>\n",
       "      <th>...</th>\n",
       "      <th>Y09</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Y11</th>\n",
       "      <th>Y12</th>\n",
       "      <th>Y13</th>\n",
       "      <th>Y14</th>\n",
       "      <th>Y15</th>\n",
       "      <th>Y16</th>\n",
       "      <th>Y17</th>\n",
       "      <th>Y18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>988.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1009.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>988.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>989.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>989.7</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>988.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>989.6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>988.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>989.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  X00    X01  X02  X03  X04     X05    X06   X07     X08  ...  Y09  Y10  \\\n",
       "0   0  9.7  988.8  1.2  0.6  0.0  1009.3  989.6  12.2  1009.9  ...  7.0  7.5   \n",
       "1   1  9.3  988.9  1.7  1.9  0.0  1009.3  989.6  12.1  1010.0  ...  6.5  7.5   \n",
       "2   2  9.4  989.0  1.1  2.3  0.0  1009.2  989.7  12.1  1010.1  ...  6.5  7.5   \n",
       "3   3  9.4  988.9  1.5  0.7  0.0  1009.2  989.6  12.0  1010.0  ...  6.0  7.0   \n",
       "4   4  9.2  988.9  0.8  1.7  0.0  1009.2  989.7  12.0  1010.1  ...  6.0  7.0   \n",
       "\n",
       "   Y11  Y12   Y13  Y14  Y15  Y16  Y17  Y18  \n",
       "0  7.0  9.0  10.0  9.5  9.0  8.0  9.0  NaN  \n",
       "1  7.0  8.5  10.0  9.5  9.0  7.5  9.0  NaN  \n",
       "2  6.5  8.0   9.5  9.5  8.5  7.5  8.5  NaN  \n",
       "3  6.0  8.0   9.5  9.0  8.5  7.5  8.5  NaN  \n",
       "4  6.0  7.5   9.5  9.0  8.5  7.5  8.5  NaN  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./AIFrenz_Season1_dataset/train.csv\")\n",
    "train_df.drop(['X14','X16','X19'], axis=1, inplace=True)\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_part = np.array(train_df[train_df.columns[1:38]].iloc[4320:], dtype=np.float32)\n",
    "target_part = np.array(train_df[train_df.columns[-1:]].iloc[4320:], dtype=np.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=True\n",
    "for idx in range(len(input_part)-1):\n",
    "    diff_x = input_part[idx+1] - input_part[idx]\n",
    "    diff_y = target_part[idx+1] - target_part[idx]\n",
    "    if first:\n",
    "        x = diff_x.reshape(1,-1)\n",
    "        y = [diff_y]\n",
    "        first=False\n",
    "    else:\n",
    "        x = np.concatenate((x, diff_x.reshape(1,-1)), axis=0)\n",
    "        y.append(diff_y)\n",
    "y = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = x[:144*2-1]\n",
    "train_target = y[:144*2-1]\n",
    "val_input = x[144*2-1:]\n",
    "val_target = y[144*2-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287, 37) (287,) (144, 37) (144,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, train_target.shape, val_input.shape, val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Shape is =============================================\n",
      "(287, 37) (287,) (144, 37) (144,)\n"
     ]
    }
   ],
   "source": [
    "global_min = train_input.min(axis=0)\n",
    "global_max = train_input.max(axis=0)\n",
    "global_range = global_max-global_min\n",
    "global_range = np.where(global_range==0,1,global_range)\n",
    "train_input = (train_input-global_min)/global_range\n",
    "val_input = (val_input-global_min)/global_range\n",
    "print(\"============================================= Shape is =============================================\")\n",
    "print(train_input.shape, train_target.shape, val_input.shape, val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l1: 0.377108\ttraining's l2: 0.398618\tvalid_0's l1: 0.365948\tvalid_0's l2: 0.344224\n",
      "Training until validation scores don't improve for 700 rounds\n",
      "[2]\ttraining's l1: 0.376453\ttraining's l2: 0.396592\tvalid_0's l1: 0.365783\tvalid_0's l2: 0.34296\n",
      "[3]\ttraining's l1: 0.375933\ttraining's l2: 0.394606\tvalid_0's l1: 0.365778\tvalid_0's l2: 0.34173\n",
      "[4]\ttraining's l1: 0.375456\ttraining's l2: 0.39266\tvalid_0's l1: 0.365795\tvalid_0's l2: 0.340527\n",
      "[5]\ttraining's l1: 0.375149\ttraining's l2: 0.390752\tvalid_0's l1: 0.366003\tvalid_0's l2: 0.339361\n",
      "[6]\ttraining's l1: 0.374844\ttraining's l2: 0.38888\tvalid_0's l1: 0.366227\tvalid_0's l2: 0.338266\n",
      "[7]\ttraining's l1: 0.374601\ttraining's l2: 0.387046\tvalid_0's l1: 0.366432\tvalid_0's l2: 0.337156\n",
      "[8]\ttraining's l1: 0.374351\ttraining's l2: 0.385247\tvalid_0's l1: 0.366667\tvalid_0's l2: 0.33608\n",
      "[9]\ttraining's l1: 0.374063\ttraining's l2: 0.383483\tvalid_0's l1: 0.3669\tvalid_0's l2: 0.335073\n",
      "[10]\ttraining's l1: 0.373797\ttraining's l2: 0.381754\tvalid_0's l1: 0.367089\tvalid_0's l2: 0.334055\n",
      "[11]\ttraining's l1: 0.373578\ttraining's l2: 0.380058\tvalid_0's l1: 0.367315\tvalid_0's l2: 0.333058\n",
      "[12]\ttraining's l1: 0.373326\ttraining's l2: 0.378394\tvalid_0's l1: 0.367512\tvalid_0's l2: 0.332009\n",
      "[13]\ttraining's l1: 0.373068\ttraining's l2: 0.376762\tvalid_0's l1: 0.367696\tvalid_0's l2: 0.331071\n",
      "[14]\ttraining's l1: 0.372871\ttraining's l2: 0.375161\tvalid_0's l1: 0.367861\tvalid_0's l2: 0.330029\n",
      "[15]\ttraining's l1: 0.37261\ttraining's l2: 0.373591\tvalid_0's l1: 0.368107\tvalid_0's l2: 0.32918\n",
      "[16]\ttraining's l1: 0.37238\ttraining's l2: 0.372051\tvalid_0's l1: 0.368243\tvalid_0's l2: 0.328195\n",
      "[17]\ttraining's l1: 0.372162\ttraining's l2: 0.370539\tvalid_0's l1: 0.368435\tvalid_0's l2: 0.327349\n",
      "[18]\ttraining's l1: 0.371934\ttraining's l2: 0.369057\tvalid_0's l1: 0.368644\tvalid_0's l2: 0.32645\n",
      "[19]\ttraining's l1: 0.371689\ttraining's l2: 0.367603\tvalid_0's l1: 0.368818\tvalid_0's l2: 0.325658\n",
      "[20]\ttraining's l1: 0.371496\ttraining's l2: 0.366176\tvalid_0's l1: 0.368958\tvalid_0's l2: 0.32476\n",
      "[21]\ttraining's l1: 0.371255\ttraining's l2: 0.364776\tvalid_0's l1: 0.369207\tvalid_0's l2: 0.324049\n",
      "[22]\ttraining's l1: 0.370937\ttraining's l2: 0.363317\tvalid_0's l1: 0.369345\tvalid_0's l2: 0.32321\n",
      "[23]\ttraining's l1: 0.370589\ttraining's l2: 0.361887\tvalid_0's l1: 0.369499\tvalid_0's l2: 0.322464\n",
      "[24]\ttraining's l1: 0.370309\ttraining's l2: 0.360484\tvalid_0's l1: 0.369722\tvalid_0's l2: 0.321668\n",
      "[25]\ttraining's l1: 0.369984\ttraining's l2: 0.359108\tvalid_0's l1: 0.369954\tvalid_0's l2: 0.320964\n",
      "[26]\ttraining's l1: 0.369507\ttraining's l2: 0.357478\tvalid_0's l1: 0.369983\tvalid_0's l2: 0.320328\n",
      "[27]\ttraining's l1: 0.369232\ttraining's l2: 0.356202\tvalid_0's l1: 0.370229\tvalid_0's l2: 0.319784\n",
      "[28]\ttraining's l1: 0.368745\ttraining's l2: 0.354621\tvalid_0's l1: 0.370337\tvalid_0's l2: 0.319295\n",
      "[29]\ttraining's l1: 0.368467\ttraining's l2: 0.353387\tvalid_0's l1: 0.370572\tvalid_0's l2: 0.318779\n",
      "[30]\ttraining's l1: 0.368209\ttraining's l2: 0.352121\tvalid_0's l1: 0.370801\tvalid_0's l2: 0.318083\n",
      "[31]\ttraining's l1: 0.367784\ttraining's l2: 0.350658\tvalid_0's l1: 0.371024\tvalid_0's l2: 0.317757\n",
      "[32]\ttraining's l1: 0.367407\ttraining's l2: 0.349437\tvalid_0's l1: 0.371177\tvalid_0's l2: 0.317175\n",
      "[33]\ttraining's l1: 0.36683\ttraining's l2: 0.348057\tvalid_0's l1: 0.371334\tvalid_0's l2: 0.316914\n",
      "[34]\ttraining's l1: 0.366478\ttraining's l2: 0.346873\tvalid_0's l1: 0.371444\tvalid_0's l2: 0.316243\n",
      "[35]\ttraining's l1: 0.365907\ttraining's l2: 0.345535\tvalid_0's l1: 0.3716\tvalid_0's l2: 0.316006\n",
      "[36]\ttraining's l1: 0.36554\ttraining's l2: 0.344387\tvalid_0's l1: 0.371747\tvalid_0's l2: 0.315475\n",
      "[37]\ttraining's l1: 0.365\ttraining's l2: 0.343088\tvalid_0's l1: 0.371935\tvalid_0's l2: 0.31526\n",
      "[38]\ttraining's l1: 0.36454\ttraining's l2: 0.34165\tvalid_0's l1: 0.372402\tvalid_0's l2: 0.315133\n",
      "[39]\ttraining's l1: 0.364152\ttraining's l2: 0.340289\tvalid_0's l1: 0.372634\tvalid_0's l2: 0.314671\n",
      "[40]\ttraining's l1: 0.363967\ttraining's l2: 0.339299\tvalid_0's l1: 0.372766\tvalid_0's l2: 0.314284\n",
      "[41]\ttraining's l1: 0.363361\ttraining's l2: 0.33784\tvalid_0's l1: 0.373076\tvalid_0's l2: 0.314077\n",
      "[42]\ttraining's l1: 0.362958\ttraining's l2: 0.336716\tvalid_0's l1: 0.373283\tvalid_0's l2: 0.31367\n",
      "[43]\ttraining's l1: 0.3624\ttraining's l2: 0.33534\tvalid_0's l1: 0.37374\tvalid_0's l2: 0.313574\n",
      "[44]\ttraining's l1: 0.362101\ttraining's l2: 0.334226\tvalid_0's l1: 0.374158\tvalid_0's l2: 0.313534\n",
      "[45]\ttraining's l1: 0.361612\ttraining's l2: 0.333013\tvalid_0's l1: 0.374416\tvalid_0's l2: 0.31329\n",
      "[46]\ttraining's l1: 0.36131\ttraining's l2: 0.331877\tvalid_0's l1: 0.374588\tvalid_0's l2: 0.312925\n",
      "[47]\ttraining's l1: 0.360768\ttraining's l2: 0.330547\tvalid_0's l1: 0.374984\tvalid_0's l2: 0.312805\n",
      "[48]\ttraining's l1: 0.359818\ttraining's l2: 0.328881\tvalid_0's l1: 0.374093\tvalid_0's l2: 0.311557\n",
      "[49]\ttraining's l1: 0.359306\ttraining's l2: 0.327642\tvalid_0's l1: 0.374285\tvalid_0's l2: 0.311312\n",
      "[50]\ttraining's l1: 0.358396\ttraining's l2: 0.326017\tvalid_0's l1: 0.373468\tvalid_0's l2: 0.3101\n",
      "[51]\ttraining's l1: 0.357858\ttraining's l2: 0.324759\tvalid_0's l1: 0.373679\tvalid_0's l2: 0.309768\n",
      "[52]\ttraining's l1: 0.357396\ttraining's l2: 0.323573\tvalid_0's l1: 0.373916\tvalid_0's l2: 0.309556\n",
      "[53]\ttraining's l1: 0.356533\ttraining's l2: 0.321995\tvalid_0's l1: 0.373173\tvalid_0's l2: 0.308384\n",
      "[54]\ttraining's l1: 0.356078\ttraining's l2: 0.32084\tvalid_0's l1: 0.373441\tvalid_0's l2: 0.308189\n",
      "[55]\ttraining's l1: 0.355237\ttraining's l2: 0.319328\tvalid_0's l1: 0.372671\tvalid_0's l2: 0.307072\n",
      "[56]\ttraining's l1: 0.354768\ttraining's l2: 0.318103\tvalid_0's l1: 0.372277\tvalid_0's l2: 0.306348\n",
      "[57]\ttraining's l1: 0.354305\ttraining's l2: 0.316901\tvalid_0's l1: 0.371886\tvalid_0's l2: 0.305642\n",
      "[58]\ttraining's l1: 0.353739\ttraining's l2: 0.315684\tvalid_0's l1: 0.372167\tvalid_0's l2: 0.305369\n",
      "[59]\ttraining's l1: 0.353286\ttraining's l2: 0.314514\tvalid_0's l1: 0.37178\tvalid_0's l2: 0.304686\n",
      "[60]\ttraining's l1: 0.352775\ttraining's l2: 0.31326\tvalid_0's l1: 0.372189\tvalid_0's l2: 0.304585\n",
      "[61]\ttraining's l1: 0.352311\ttraining's l2: 0.312122\tvalid_0's l1: 0.371807\tvalid_0's l2: 0.303927\n",
      "[62]\ttraining's l1: 0.351804\ttraining's l2: 0.310932\tvalid_0's l1: 0.372171\tvalid_0's l2: 0.303786\n",
      "[63]\ttraining's l1: 0.351346\ttraining's l2: 0.309825\tvalid_0's l1: 0.371792\tvalid_0's l2: 0.303148\n",
      "[64]\ttraining's l1: 0.350888\ttraining's l2: 0.308775\tvalid_0's l1: 0.371991\tvalid_0's l2: 0.302865\n",
      "[65]\ttraining's l1: 0.350436\ttraining's l2: 0.307698\tvalid_0's l1: 0.371616\tvalid_0's l2: 0.302248\n",
      "[66]\ttraining's l1: 0.35003\ttraining's l2: 0.306724\tvalid_0's l1: 0.371827\tvalid_0's l2: 0.302079\n",
      "[67]\ttraining's l1: 0.349179\ttraining's l2: 0.305329\tvalid_0's l1: 0.371093\tvalid_0's l2: 0.301076\n",
      "[68]\ttraining's l1: 0.348736\ttraining's l2: 0.30429\tvalid_0's l1: 0.370723\tvalid_0's l2: 0.300486\n",
      "[69]\ttraining's l1: 0.348315\ttraining's l2: 0.303271\tvalid_0's l1: 0.370355\tvalid_0's l2: 0.299908\n",
      "[70]\ttraining's l1: 0.347829\ttraining's l2: 0.302176\tvalid_0's l1: 0.370707\tvalid_0's l2: 0.299817\n",
      "[71]\ttraining's l1: 0.347396\ttraining's l2: 0.301185\tvalid_0's l1: 0.370344\tvalid_0's l2: 0.299261\n",
      "[72]\ttraining's l1: 0.346957\ttraining's l2: 0.30022\tvalid_0's l1: 0.370534\tvalid_0's l2: 0.299029\n",
      "[73]\ttraining's l1: 0.346517\ttraining's l2: 0.299251\tvalid_0's l1: 0.370284\tvalid_0's l2: 0.298579\n",
      "[74]\ttraining's l1: 0.345935\ttraining's l2: 0.298114\tvalid_0's l1: 0.370659\tvalid_0's l2: 0.298531\n",
      "[75]\ttraining's l1: 0.345514\ttraining's l2: 0.297171\tvalid_0's l1: 0.370411\tvalid_0's l2: 0.298096\n",
      "[76]\ttraining's l1: 0.344692\ttraining's l2: 0.295867\tvalid_0's l1: 0.369703\tvalid_0's l2: 0.297166\n",
      "[77]\ttraining's l1: 0.344273\ttraining's l2: 0.294951\tvalid_0's l1: 0.369487\tvalid_0's l2: 0.296749\n",
      "[78]\ttraining's l1: 0.343792\ttraining's l2: 0.294002\tvalid_0's l1: 0.369687\tvalid_0's l2: 0.29666\n",
      "[79]\ttraining's l1: 0.343378\ttraining's l2: 0.29311\tvalid_0's l1: 0.369486\tvalid_0's l2: 0.296258\n",
      "[80]\ttraining's l1: 0.342906\ttraining's l2: 0.292253\tvalid_0's l1: 0.369554\tvalid_0's l2: 0.295953\n",
      "[81]\ttraining's l1: 0.342497\ttraining's l2: 0.291383\tvalid_0's l1: 0.369365\tvalid_0's l2: 0.295565\n",
      "[82]\ttraining's l1: 0.341944\ttraining's l2: 0.290341\tvalid_0's l1: 0.369046\tvalid_0's l2: 0.29507\n",
      "[83]\ttraining's l1: 0.341597\ttraining's l2: 0.289498\tvalid_0's l1: 0.368854\tvalid_0's l2: 0.294691\n",
      "[84]\ttraining's l1: 0.341098\ttraining's l2: 0.288573\tvalid_0's l1: 0.369071\tvalid_0's l2: 0.294554\n",
      "[85]\ttraining's l1: 0.340448\ttraining's l2: 0.28731\tvalid_0's l1: 0.368861\tvalid_0's l2: 0.294369\n",
      "[86]\ttraining's l1: 0.340004\ttraining's l2: 0.286471\tvalid_0's l1: 0.368577\tvalid_0's l2: 0.293942\n",
      "[87]\ttraining's l1: 0.339371\ttraining's l2: 0.285239\tvalid_0's l1: 0.368375\tvalid_0's l2: 0.29377\n",
      "[88]\ttraining's l1: 0.339029\ttraining's l2: 0.284445\tvalid_0's l1: 0.368169\tvalid_0's l2: 0.293425\n",
      "[89]\ttraining's l1: 0.338302\ttraining's l2: 0.283148\tvalid_0's l1: 0.368066\tvalid_0's l2: 0.293052\n",
      "[90]\ttraining's l1: 0.337877\ttraining's l2: 0.282354\tvalid_0's l1: 0.367807\tvalid_0's l2: 0.292635\n",
      "[91]\ttraining's l1: 0.337137\ttraining's l2: 0.281082\tvalid_0's l1: 0.367717\tvalid_0's l2: 0.292309\n",
      "[92]\ttraining's l1: 0.336486\ttraining's l2: 0.279901\tvalid_0's l1: 0.367633\tvalid_0's l2: 0.292221\n",
      "[93]\ttraining's l1: 0.335929\ttraining's l2: 0.278946\tvalid_0's l1: 0.367473\tvalid_0's l2: 0.29191\n",
      "[94]\ttraining's l1: 0.335256\ttraining's l2: 0.27772\tvalid_0's l1: 0.367448\tvalid_0's l2: 0.291579\n",
      "[95]\ttraining's l1: 0.334615\ttraining's l2: 0.276576\tvalid_0's l1: 0.367376\tvalid_0's l2: 0.291507\n",
      "[96]\ttraining's l1: 0.334137\ttraining's l2: 0.275645\tvalid_0's l1: 0.367402\tvalid_0's l2: 0.291288\n",
      "[97]\ttraining's l1: 0.333521\ttraining's l2: 0.274455\tvalid_0's l1: 0.367355\tvalid_0's l2: 0.291156\n",
      "[98]\ttraining's l1: 0.33312\ttraining's l2: 0.273728\tvalid_0's l1: 0.367145\tvalid_0's l2: 0.290788\n",
      "[99]\ttraining's l1: 0.332516\ttraining's l2: 0.272567\tvalid_0's l1: 0.367125\tvalid_0's l2: 0.290495\n",
      "[100]\ttraining's l1: 0.331879\ttraining's l2: 0.27147\tvalid_0's l1: 0.367047\tvalid_0's l2: 0.290443\n",
      "[101]\ttraining's l1: 0.331447\ttraining's l2: 0.270589\tvalid_0's l1: 0.367052\tvalid_0's l2: 0.290274\n",
      "[102]\ttraining's l1: 0.330873\ttraining's l2: 0.269463\tvalid_0's l1: 0.367068\tvalid_0's l2: 0.290174\n",
      "[103]\ttraining's l1: 0.330251\ttraining's l2: 0.2684\tvalid_0's l1: 0.367068\tvalid_0's l2: 0.290127\n",
      "[104]\ttraining's l1: 0.329838\ttraining's l2: 0.267548\tvalid_0's l1: 0.36708\tvalid_0's l2: 0.289974\n",
      "[105]\ttraining's l1: 0.329197\ttraining's l2: 0.266457\tvalid_0's l1: 0.367281\tvalid_0's l2: 0.289844\n",
      "[106]\ttraining's l1: 0.328798\ttraining's l2: 0.265749\tvalid_0's l1: 0.367061\tvalid_0's l2: 0.289516\n",
      "[107]\ttraining's l1: 0.328191\ttraining's l2: 0.264713\tvalid_0's l1: 0.367254\tvalid_0's l2: 0.289452\n",
      "[108]\ttraining's l1: 0.327578\ttraining's l2: 0.263701\tvalid_0's l1: 0.367239\tvalid_0's l2: 0.289434\n",
      "[109]\ttraining's l1: 0.327215\ttraining's l2: 0.263038\tvalid_0's l1: 0.367086\tvalid_0's l2: 0.289121\n",
      "[110]\ttraining's l1: 0.326264\ttraining's l2: 0.261739\tvalid_0's l1: 0.36743\tvalid_0's l2: 0.289025\n",
      "[111]\ttraining's l1: 0.325691\ttraining's l2: 0.26097\tvalid_0's l1: 0.367514\tvalid_0's l2: 0.288846\n",
      "[112]\ttraining's l1: 0.325068\ttraining's l2: 0.259979\tvalid_0's l1: 0.367723\tvalid_0's l2: 0.288635\n",
      "[113]\ttraining's l1: 0.324519\ttraining's l2: 0.259017\tvalid_0's l1: 0.367707\tvalid_0's l2: 0.288631\n",
      "[114]\ttraining's l1: 0.324146\ttraining's l2: 0.258233\tvalid_0's l1: 0.367734\tvalid_0's l2: 0.288516\n",
      "[115]\ttraining's l1: 0.323633\ttraining's l2: 0.257496\tvalid_0's l1: 0.367818\tvalid_0's l2: 0.288353\n",
      "[116]\ttraining's l1: 0.322711\ttraining's l2: 0.256255\tvalid_0's l1: 0.368153\tvalid_0's l2: 0.288287\n",
      "[117]\ttraining's l1: 0.322223\ttraining's l2: 0.255335\tvalid_0's l1: 0.368145\tvalid_0's l2: 0.288293\n",
      "[118]\ttraining's l1: 0.321615\ttraining's l2: 0.254394\tvalid_0's l1: 0.36837\tvalid_0's l2: 0.288107\n",
      "[119]\ttraining's l1: 0.321133\ttraining's l2: 0.253687\tvalid_0's l1: 0.368492\tvalid_0's l2: 0.287957\n",
      "[120]\ttraining's l1: 0.320769\ttraining's l2: 0.252941\tvalid_0's l1: 0.368517\tvalid_0's l2: 0.287862\n",
      "[121]\ttraining's l1: 0.31995\ttraining's l2: 0.251777\tvalid_0's l1: 0.368972\tvalid_0's l2: 0.287851\n",
      "[122]\ttraining's l1: 0.319476\ttraining's l2: 0.250896\tvalid_0's l1: 0.368937\tvalid_0's l2: 0.287848\n",
      "[123]\ttraining's l1: 0.319028\ttraining's l2: 0.250032\tvalid_0's l1: 0.368947\tvalid_0's l2: 0.287838\n",
      "[124]\ttraining's l1: 0.318604\ttraining's l2: 0.249394\tvalid_0's l1: 0.368998\tvalid_0's l2: 0.287794\n",
      "[125]\ttraining's l1: 0.31826\ttraining's l2: 0.24868\tvalid_0's l1: 0.369032\tvalid_0's l2: 0.287716\n",
      "[126]\ttraining's l1: 0.317688\ttraining's l2: 0.247792\tvalid_0's l1: 0.369209\tvalid_0's l2: 0.287536\n",
      "[127]\ttraining's l1: 0.316838\ttraining's l2: 0.246658\tvalid_0's l1: 0.369574\tvalid_0's l2: 0.287521\n",
      "[128]\ttraining's l1: 0.316415\ttraining's l2: 0.245924\tvalid_0's l1: 0.369491\tvalid_0's l2: 0.287419\n",
      "[129]\ttraining's l1: 0.316\ttraining's l2: 0.245312\tvalid_0's l1: 0.36955\tvalid_0's l2: 0.287385\n",
      "[130]\ttraining's l1: 0.31541\ttraining's l2: 0.244442\tvalid_0's l1: 0.369777\tvalid_0's l2: 0.287197\n",
      "[131]\ttraining's l1: 0.315081\ttraining's l2: 0.243761\tvalid_0's l1: 0.369833\tvalid_0's l2: 0.287137\n",
      "[132]\ttraining's l1: 0.314295\ttraining's l2: 0.242663\tvalid_0's l1: 0.370194\tvalid_0's l2: 0.287145\n",
      "[133]\ttraining's l1: 0.31388\ttraining's l2: 0.241961\tvalid_0's l1: 0.370128\tvalid_0's l2: 0.287058\n",
      "[134]\ttraining's l1: 0.313215\ttraining's l2: 0.240982\tvalid_0's l1: 0.370101\tvalid_0's l2: 0.287125\n",
      "[135]\ttraining's l1: 0.31269\ttraining's l2: 0.240224\tvalid_0's l1: 0.370239\tvalid_0's l2: 0.287023\n",
      "[136]\ttraining's l1: 0.31228\ttraining's l2: 0.239542\tvalid_0's l1: 0.370209\tvalid_0's l2: 0.286975\n",
      "[137]\ttraining's l1: 0.311625\ttraining's l2: 0.238592\tvalid_0's l1: 0.370216\tvalid_0's l2: 0.287055\n",
      "[138]\ttraining's l1: 0.311229\ttraining's l2: 0.238023\tvalid_0's l1: 0.370264\tvalid_0's l2: 0.287038\n",
      "[139]\ttraining's l1: 0.310545\ttraining's l2: 0.237009\tvalid_0's l1: 0.37075\tvalid_0's l2: 0.287132\n",
      "[140]\ttraining's l1: 0.309901\ttraining's l2: 0.236088\tvalid_0's l1: 0.370792\tvalid_0's l2: 0.28722\n",
      "[141]\ttraining's l1: 0.309366\ttraining's l2: 0.235329\tvalid_0's l1: 0.370839\tvalid_0's l2: 0.287241\n",
      "[142]\ttraining's l1: 0.308876\ttraining's l2: 0.234609\tvalid_0's l1: 0.370997\tvalid_0's l2: 0.28716\n",
      "[143]\ttraining's l1: 0.308225\ttraining's l2: 0.23366\tvalid_0's l1: 0.371038\tvalid_0's l2: 0.287079\n",
      "[144]\ttraining's l1: 0.307514\ttraining's l2: 0.232589\tvalid_0's l1: 0.371259\tvalid_0's l2: 0.28698\n",
      "[145]\ttraining's l1: 0.306863\ttraining's l2: 0.231642\tvalid_0's l1: 0.371266\tvalid_0's l2: 0.286902\n",
      "[146]\ttraining's l1: 0.306161\ttraining's l2: 0.230596\tvalid_0's l1: 0.371481\tvalid_0's l2: 0.286814\n",
      "[147]\ttraining's l1: 0.305842\ttraining's l2: 0.229979\tvalid_0's l1: 0.37157\tvalid_0's l2: 0.286767\n",
      "[148]\ttraining's l1: 0.305221\ttraining's l2: 0.229078\tvalid_0's l1: 0.371616\tvalid_0's l2: 0.286716\n",
      "[149]\ttraining's l1: 0.304528\ttraining's l2: 0.22806\tvalid_0's l1: 0.371826\tvalid_0's l2: 0.286641\n",
      "[150]\ttraining's l1: 0.304036\ttraining's l2: 0.227392\tvalid_0's l1: 0.372078\tvalid_0's l2: 0.286747\n",
      "[151]\ttraining's l1: 0.303414\ttraining's l2: 0.226496\tvalid_0's l1: 0.372085\tvalid_0's l2: 0.286699\n",
      "[152]\ttraining's l1: 0.302708\ttraining's l2: 0.225506\tvalid_0's l1: 0.372195\tvalid_0's l2: 0.286691\n",
      "[153]\ttraining's l1: 0.302108\ttraining's l2: 0.224652\tvalid_0's l1: 0.372244\tvalid_0's l2: 0.286655\n",
      "[154]\ttraining's l1: 0.301477\ttraining's l2: 0.223842\tvalid_0's l1: 0.372002\tvalid_0's l2: 0.286256\n",
      "[155]\ttraining's l1: 0.301168\ttraining's l2: 0.223263\tvalid_0's l1: 0.372081\tvalid_0's l2: 0.286219\n",
      "[156]\ttraining's l1: 0.300492\ttraining's l2: 0.2223\tvalid_0's l1: 0.372256\tvalid_0's l2: 0.286151\n",
      "[157]\ttraining's l1: 0.30014\ttraining's l2: 0.221722\tvalid_0's l1: 0.372271\tvalid_0's l2: 0.286057\n",
      "[158]\ttraining's l1: 0.299557\ttraining's l2: 0.220884\tvalid_0's l1: 0.372277\tvalid_0's l2: 0.286033\n",
      "[159]\ttraining's l1: 0.298814\ttraining's l2: 0.219878\tvalid_0's l1: 0.372412\tvalid_0's l2: 0.285846\n",
      "[160]\ttraining's l1: 0.298216\ttraining's l2: 0.219029\tvalid_0's l1: 0.371978\tvalid_0's l2: 0.285494\n",
      "[161]\ttraining's l1: 0.297906\ttraining's l2: 0.21848\tvalid_0's l1: 0.372031\tvalid_0's l2: 0.285467\n",
      "[162]\ttraining's l1: 0.297487\ttraining's l2: 0.217918\tvalid_0's l1: 0.372013\tvalid_0's l2: 0.285353\n",
      "[163]\ttraining's l1: 0.297023\ttraining's l2: 0.217192\tvalid_0's l1: 0.372278\tvalid_0's l2: 0.285384\n",
      "[164]\ttraining's l1: 0.296438\ttraining's l2: 0.216366\tvalid_0's l1: 0.371858\tvalid_0's l2: 0.285044\n",
      "[165]\ttraining's l1: 0.295759\ttraining's l2: 0.215401\tvalid_0's l1: 0.372024\tvalid_0's l2: 0.284877\n",
      "[166]\ttraining's l1: 0.295415\ttraining's l2: 0.214851\tvalid_0's l1: 0.372257\tvalid_0's l2: 0.28474\n",
      "[167]\ttraining's l1: 0.294974\ttraining's l2: 0.214265\tvalid_0's l1: 0.372411\tvalid_0's l2: 0.284778\n",
      "[168]\ttraining's l1: 0.294386\ttraining's l2: 0.213525\tvalid_0's l1: 0.372193\tvalid_0's l2: 0.28442\n",
      "[169]\ttraining's l1: 0.293726\ttraining's l2: 0.212587\tvalid_0's l1: 0.372364\tvalid_0's l2: 0.284266\n",
      "[170]\ttraining's l1: 0.29339\ttraining's l2: 0.212056\tvalid_0's l1: 0.372593\tvalid_0's l2: 0.28414\n",
      "[171]\ttraining's l1: 0.292829\ttraining's l2: 0.211285\tvalid_0's l1: 0.372311\tvalid_0's l2: 0.283866\n",
      "[172]\ttraining's l1: 0.292502\ttraining's l2: 0.210767\tvalid_0's l1: 0.372539\tvalid_0's l2: 0.283723\n",
      "[173]\ttraining's l1: 0.291858\ttraining's l2: 0.209855\tvalid_0's l1: 0.372742\tvalid_0's l2: 0.283616\n",
      "[174]\ttraining's l1: 0.291537\ttraining's l2: 0.209349\tvalid_0's l1: 0.372996\tvalid_0's l2: 0.283505\n",
      "[175]\ttraining's l1: 0.290962\ttraining's l2: 0.208646\tvalid_0's l1: 0.372837\tvalid_0's l2: 0.283174\n",
      "[176]\ttraining's l1: 0.290645\ttraining's l2: 0.208152\tvalid_0's l1: 0.37306\tvalid_0's l2: 0.283046\n",
      "[177]\ttraining's l1: 0.290037\ttraining's l2: 0.207338\tvalid_0's l1: 0.373218\tvalid_0's l2: 0.282946\n",
      "[178]\ttraining's l1: 0.289757\ttraining's l2: 0.20685\tvalid_0's l1: 0.373509\tvalid_0's l2: 0.282867\n",
      "[179]\ttraining's l1: 0.289129\ttraining's l2: 0.205973\tvalid_0's l1: 0.373707\tvalid_0's l2: 0.282777\n",
      "[180]\ttraining's l1: 0.288814\ttraining's l2: 0.2055\tvalid_0's l1: 0.373923\tvalid_0's l2: 0.282662\n",
      "[181]\ttraining's l1: 0.288323\ttraining's l2: 0.204809\tvalid_0's l1: 0.373691\tvalid_0's l2: 0.282396\n",
      "[182]\ttraining's l1: 0.287676\ttraining's l2: 0.203978\tvalid_0's l1: 0.373811\tvalid_0's l2: 0.282161\n",
      "[183]\ttraining's l1: 0.287392\ttraining's l2: 0.203513\tvalid_0's l1: 0.374087\tvalid_0's l2: 0.282111\n",
      "[184]\ttraining's l1: 0.286942\ttraining's l2: 0.202877\tvalid_0's l1: 0.374213\tvalid_0's l2: 0.28222\n",
      "[185]\ttraining's l1: 0.286642\ttraining's l2: 0.202425\tvalid_0's l1: 0.374426\tvalid_0's l2: 0.282117\n",
      "[186]\ttraining's l1: 0.286046\ttraining's l2: 0.201589\tvalid_0's l1: 0.374565\tvalid_0's l2: 0.282011\n",
      "[187]\ttraining's l1: 0.28554\ttraining's l2: 0.200862\tvalid_0's l1: 0.374434\tvalid_0's l2: 0.281922\n",
      "[188]\ttraining's l1: 0.28527\ttraining's l2: 0.200417\tvalid_0's l1: 0.374703\tvalid_0's l2: 0.281883\n",
      "[189]\ttraining's l1: 0.284658\ttraining's l2: 0.199626\tvalid_0's l1: 0.37482\tvalid_0's l2: 0.28167\n",
      "[190]\ttraining's l1: 0.284282\ttraining's l2: 0.199143\tvalid_0's l1: 0.375111\tvalid_0's l2: 0.281692\n",
      "[191]\ttraining's l1: 0.283706\ttraining's l2: 0.198341\tvalid_0's l1: 0.375187\tvalid_0's l2: 0.281656\n",
      "[192]\ttraining's l1: 0.283425\ttraining's l2: 0.197885\tvalid_0's l1: 0.375457\tvalid_0's l2: 0.281611\n",
      "[193]\ttraining's l1: 0.282974\ttraining's l2: 0.197188\tvalid_0's l1: 0.375343\tvalid_0's l2: 0.281438\n",
      "[194]\ttraining's l1: 0.282712\ttraining's l2: 0.196745\tvalid_0's l1: 0.375571\tvalid_0's l2: 0.281363\n",
      "[195]\ttraining's l1: 0.282084\ttraining's l2: 0.195978\tvalid_0's l1: 0.375611\tvalid_0's l2: 0.28113\n",
      "[196]\ttraining's l1: 0.281828\ttraining's l2: 0.195546\tvalid_0's l1: 0.375868\tvalid_0's l2: 0.281094\n",
      "[197]\ttraining's l1: 0.281363\ttraining's l2: 0.194893\tvalid_0's l1: 0.375733\tvalid_0's l2: 0.28088\n",
      "[198]\ttraining's l1: 0.281122\ttraining's l2: 0.19447\tvalid_0's l1: 0.375989\tvalid_0's l2: 0.280848\n",
      "[199]\ttraining's l1: 0.280537\ttraining's l2: 0.193708\tvalid_0's l1: 0.37607\tvalid_0's l2: 0.280783\n",
      "[200]\ttraining's l1: 0.280291\ttraining's l2: 0.19326\tvalid_0's l1: 0.37638\tvalid_0's l2: 0.280903\n",
      "[201]\ttraining's l1: 0.279859\ttraining's l2: 0.192602\tvalid_0's l1: 0.376274\tvalid_0's l2: 0.280752\n",
      "[202]\ttraining's l1: 0.279678\ttraining's l2: 0.192194\tvalid_0's l1: 0.376532\tvalid_0's l2: 0.280712\n",
      "[203]\ttraining's l1: 0.2793\ttraining's l2: 0.191619\tvalid_0's l1: 0.376612\tvalid_0's l2: 0.280814\n",
      "[204]\ttraining's l1: 0.278719\ttraining's l2: 0.190893\tvalid_0's l1: 0.376651\tvalid_0's l2: 0.280602\n",
      "[205]\ttraining's l1: 0.278538\ttraining's l2: 0.190494\tvalid_0's l1: 0.376898\tvalid_0's l2: 0.280585\n",
      "[206]\ttraining's l1: 0.27839\ttraining's l2: 0.190117\tvalid_0's l1: 0.377122\tvalid_0's l2: 0.280569\n",
      "[207]\ttraining's l1: 0.277839\ttraining's l2: 0.18939\tvalid_0's l1: 0.377144\tvalid_0's l2: 0.280568\n",
      "[208]\ttraining's l1: 0.277413\ttraining's l2: 0.188781\tvalid_0's l1: 0.377013\tvalid_0's l2: 0.280375\n",
      "[209]\ttraining's l1: 0.277234\ttraining's l2: 0.188398\tvalid_0's l1: 0.377257\tvalid_0's l2: 0.280365\n",
      "[210]\ttraining's l1: 0.276868\ttraining's l2: 0.187849\tvalid_0's l1: 0.377336\tvalid_0's l2: 0.280471\n",
      "[211]\ttraining's l1: 0.276635\ttraining's l2: 0.187404\tvalid_0's l1: 0.377615\tvalid_0's l2: 0.28056\n",
      "[212]\ttraining's l1: 0.276071\ttraining's l2: 0.186616\tvalid_0's l1: 0.377839\tvalid_0's l2: 0.280773\n",
      "[213]\ttraining's l1: 0.275547\ttraining's l2: 0.186009\tvalid_0's l1: 0.377781\tvalid_0's l2: 0.280776\n",
      "[214]\ttraining's l1: 0.275319\ttraining's l2: 0.185575\tvalid_0's l1: 0.378057\tvalid_0's l2: 0.280869\n",
      "[215]\ttraining's l1: 0.274757\ttraining's l2: 0.184886\tvalid_0's l1: 0.378119\tvalid_0's l2: 0.280676\n",
      "[216]\ttraining's l1: 0.274551\ttraining's l2: 0.184467\tvalid_0's l1: 0.378471\tvalid_0's l2: 0.280781\n",
      "[217]\ttraining's l1: 0.274129\ttraining's l2: 0.183889\tvalid_0's l1: 0.378402\tvalid_0's l2: 0.2806\n",
      "[218]\ttraining's l1: 0.273909\ttraining's l2: 0.183473\tvalid_0's l1: 0.378707\tvalid_0's l2: 0.2807\n",
      "[219]\ttraining's l1: 0.273375\ttraining's l2: 0.182716\tvalid_0's l1: 0.378932\tvalid_0's l2: 0.280833\n",
      "[220]\ttraining's l1: 0.273022\ttraining's l2: 0.182324\tvalid_0's l1: 0.379071\tvalid_0's l2: 0.28079\n",
      "[221]\ttraining's l1: 0.272509\ttraining's l2: 0.181743\tvalid_0's l1: 0.379059\tvalid_0's l2: 0.280799\n",
      "[222]\ttraining's l1: 0.271973\ttraining's l2: 0.181027\tvalid_0's l1: 0.379108\tvalid_0's l2: 0.28073\n",
      "[223]\ttraining's l1: 0.271774\ttraining's l2: 0.18063\tvalid_0's l1: 0.37945\tvalid_0's l2: 0.280844\n",
      "[224]\ttraining's l1: 0.271334\ttraining's l2: 0.180039\tvalid_0's l1: 0.379381\tvalid_0's l2: 0.280634\n",
      "[225]\ttraining's l1: 0.270795\ttraining's l2: 0.179308\tvalid_0's l1: 0.379602\tvalid_0's l2: 0.280776\n",
      "[226]\ttraining's l1: 0.270408\ttraining's l2: 0.178918\tvalid_0's l1: 0.37971\tvalid_0's l2: 0.280713\n",
      "[227]\ttraining's l1: 0.269927\ttraining's l2: 0.178362\tvalid_0's l1: 0.379698\tvalid_0's l2: 0.280728\n",
      "[228]\ttraining's l1: 0.269726\ttraining's l2: 0.177972\tvalid_0's l1: 0.379997\tvalid_0's l2: 0.280837\n",
      "[229]\ttraining's l1: 0.269199\ttraining's l2: 0.177322\tvalid_0's l1: 0.38013\tvalid_0's l2: 0.280821\n",
      "[230]\ttraining's l1: 0.268854\ttraining's l2: 0.176799\tvalid_0's l1: 0.380299\tvalid_0's l2: 0.280835\n",
      "[231]\ttraining's l1: 0.268456\ttraining's l2: 0.17627\tvalid_0's l1: 0.380242\tvalid_0's l2: 0.280673\n",
      "[232]\ttraining's l1: 0.268255\ttraining's l2: 0.175885\tvalid_0's l1: 0.380599\tvalid_0's l2: 0.280811\n",
      "[233]\ttraining's l1: 0.267741\ttraining's l2: 0.175202\tvalid_0's l1: 0.380612\tvalid_0's l2: 0.280816\n",
      "[234]\ttraining's l1: 0.267335\ttraining's l2: 0.174651\tvalid_0's l1: 0.380519\tvalid_0's l2: 0.280604\n",
      "[235]\ttraining's l1: 0.267051\ttraining's l2: 0.174287\tvalid_0's l1: 0.380901\tvalid_0's l2: 0.280716\n",
      "[236]\ttraining's l1: 0.26654\ttraining's l2: 0.173595\tvalid_0's l1: 0.381155\tvalid_0's l2: 0.280805\n",
      "[237]\ttraining's l1: 0.266158\ttraining's l2: 0.173232\tvalid_0's l1: 0.381285\tvalid_0's l2: 0.280768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[238]\ttraining's l1: 0.265661\ttraining's l2: 0.172568\tvalid_0's l1: 0.381334\tvalid_0's l2: 0.280718\n",
      "[239]\ttraining's l1: 0.265323\ttraining's l2: 0.172071\tvalid_0's l1: 0.381498\tvalid_0's l2: 0.280741\n",
      "[240]\ttraining's l1: 0.264868\ttraining's l2: 0.171552\tvalid_0's l1: 0.381491\tvalid_0's l2: 0.280762\n",
      "[241]\ttraining's l1: 0.26439\ttraining's l2: 0.170947\tvalid_0's l1: 0.381581\tvalid_0's l2: 0.280731\n",
      "[242]\ttraining's l1: 0.264012\ttraining's l2: 0.170462\tvalid_0's l1: 0.38154\tvalid_0's l2: 0.280757\n",
      "[243]\ttraining's l1: 0.263583\ttraining's l2: 0.169795\tvalid_0's l1: 0.381823\tvalid_0's l2: 0.280846\n",
      "[244]\ttraining's l1: 0.263239\ttraining's l2: 0.169311\tvalid_0's l1: 0.381683\tvalid_0's l2: 0.280646\n",
      "[245]\ttraining's l1: 0.263044\ttraining's l2: 0.168956\tvalid_0's l1: 0.382068\tvalid_0's l2: 0.280799\n",
      "[246]\ttraining's l1: 0.262537\ttraining's l2: 0.168318\tvalid_0's l1: 0.382081\tvalid_0's l2: 0.280817\n",
      "[247]\ttraining's l1: 0.26211\ttraining's l2: 0.167669\tvalid_0's l1: 0.382359\tvalid_0's l2: 0.280914\n",
      "[248]\ttraining's l1: 0.261783\ttraining's l2: 0.167267\tvalid_0's l1: 0.382379\tvalid_0's l2: 0.280903\n",
      "[249]\ttraining's l1: 0.26138\ttraining's l2: 0.166781\tvalid_0's l1: 0.382404\tvalid_0's l2: 0.28094\n",
      "[250]\ttraining's l1: 0.261007\ttraining's l2: 0.166316\tvalid_0's l1: 0.382363\tvalid_0's l2: 0.280973\n",
      "[251]\ttraining's l1: 0.26052\ttraining's l2: 0.165715\tvalid_0's l1: 0.38248\tvalid_0's l2: 0.28096\n",
      "[252]\ttraining's l1: 0.260329\ttraining's l2: 0.165373\tvalid_0's l1: 0.38286\tvalid_0's l2: 0.281123\n",
      "[253]\ttraining's l1: 0.260008\ttraining's l2: 0.164914\tvalid_0's l1: 0.382723\tvalid_0's l2: 0.280931\n",
      "[254]\ttraining's l1: 0.259827\ttraining's l2: 0.164574\tvalid_0's l1: 0.383057\tvalid_0's l2: 0.28108\n",
      "[255]\ttraining's l1: 0.259348\ttraining's l2: 0.163962\tvalid_0's l1: 0.383068\tvalid_0's l2: 0.281105\n",
      "[256]\ttraining's l1: 0.258944\ttraining's l2: 0.163341\tvalid_0's l1: 0.38334\tvalid_0's l2: 0.281213\n",
      "[257]\ttraining's l1: 0.258606\ttraining's l2: 0.162955\tvalid_0's l1: 0.383359\tvalid_0's l2: 0.281206\n",
      "[258]\ttraining's l1: 0.258124\ttraining's l2: 0.162344\tvalid_0's l1: 0.383619\tvalid_0's l2: 0.281392\n",
      "[259]\ttraining's l1: 0.25781\ttraining's l2: 0.161964\tvalid_0's l1: 0.383679\tvalid_0's l2: 0.281364\n",
      "[260]\ttraining's l1: 0.257423\ttraining's l2: 0.161333\tvalid_0's l1: 0.383648\tvalid_0's l2: 0.281279\n",
      "[261]\ttraining's l1: 0.25705\ttraining's l2: 0.160876\tvalid_0's l1: 0.383664\tvalid_0's l2: 0.281321\n",
      "[262]\ttraining's l1: 0.256878\ttraining's l2: 0.160544\tvalid_0's l1: 0.383865\tvalid_0's l2: 0.281434\n",
      "[263]\ttraining's l1: 0.256454\ttraining's l2: 0.159994\tvalid_0's l1: 0.383999\tvalid_0's l2: 0.28139\n",
      "[264]\ttraining's l1: 0.256053\ttraining's l2: 0.159552\tvalid_0's l1: 0.383974\tvalid_0's l2: 0.281498\n",
      "[265]\ttraining's l1: 0.255792\ttraining's l2: 0.159234\tvalid_0's l1: 0.384332\tvalid_0's l2: 0.281619\n",
      "[266]\ttraining's l1: 0.255489\ttraining's l2: 0.158801\tvalid_0's l1: 0.384221\tvalid_0's l2: 0.281434\n",
      "[267]\ttraining's l1: 0.25532\ttraining's l2: 0.158481\tvalid_0's l1: 0.384408\tvalid_0's l2: 0.281549\n",
      "[268]\ttraining's l1: 0.254945\ttraining's l2: 0.157875\tvalid_0's l1: 0.384384\tvalid_0's l2: 0.281473\n",
      "[269]\ttraining's l1: 0.254633\ttraining's l2: 0.157513\tvalid_0's l1: 0.384423\tvalid_0's l2: 0.281449\n",
      "[270]\ttraining's l1: 0.254275\ttraining's l2: 0.157064\tvalid_0's l1: 0.384407\tvalid_0's l2: 0.281546\n",
      "[271]\ttraining's l1: 0.254003\ttraining's l2: 0.156716\tvalid_0's l1: 0.384579\tvalid_0's l2: 0.281669\n",
      "[272]\ttraining's l1: 0.253667\ttraining's l2: 0.15629\tvalid_0's l1: 0.384548\tvalid_0's l2: 0.281711\n",
      "[273]\ttraining's l1: 0.25334\ttraining's l2: 0.155868\tvalid_0's l1: 0.384538\tvalid_0's l2: 0.28158\n",
      "[274]\ttraining's l1: 0.252983\ttraining's l2: 0.155472\tvalid_0's l1: 0.384721\tvalid_0's l2: 0.281667\n",
      "[275]\ttraining's l1: 0.252569\ttraining's l2: 0.15495\tvalid_0's l1: 0.384954\tvalid_0's l2: 0.281944\n",
      "[276]\ttraining's l1: 0.252127\ttraining's l2: 0.154415\tvalid_0's l1: 0.385077\tvalid_0's l2: 0.281951\n",
      "[277]\ttraining's l1: 0.25174\ttraining's l2: 0.154\tvalid_0's l1: 0.385046\tvalid_0's l2: 0.282062\n",
      "[278]\ttraining's l1: 0.251405\ttraining's l2: 0.153601\tvalid_0's l1: 0.384968\tvalid_0's l2: 0.281974\n",
      "[279]\ttraining's l1: 0.251139\ttraining's l2: 0.153293\tvalid_0's l1: 0.385292\tvalid_0's l2: 0.282204\n",
      "[280]\ttraining's l1: 0.250791\ttraining's l2: 0.152858\tvalid_0's l1: 0.385331\tvalid_0's l2: 0.282136\n",
      "[281]\ttraining's l1: 0.250424\ttraining's l2: 0.152301\tvalid_0's l1: 0.385632\tvalid_0's l2: 0.282259\n",
      "[282]\ttraining's l1: 0.250136\ttraining's l2: 0.151912\tvalid_0's l1: 0.385651\tvalid_0's l2: 0.282231\n",
      "[283]\ttraining's l1: 0.249773\ttraining's l2: 0.151368\tvalid_0's l1: 0.385949\tvalid_0's l2: 0.28236\n",
      "[284]\ttraining's l1: 0.249479\ttraining's l2: 0.150928\tvalid_0's l1: 0.386207\tvalid_0's l2: 0.282597\n",
      "[285]\ttraining's l1: 0.249132\ttraining's l2: 0.150511\tvalid_0's l1: 0.386193\tvalid_0's l2: 0.282696\n",
      "[286]\ttraining's l1: 0.248797\ttraining's l2: 0.150147\tvalid_0's l1: 0.386294\tvalid_0's l2: 0.282782\n",
      "[287]\ttraining's l1: 0.248423\ttraining's l2: 0.14975\tvalid_0's l1: 0.386263\tvalid_0's l2: 0.282895\n",
      "[288]\ttraining's l1: 0.24808\ttraining's l2: 0.149194\tvalid_0's l1: 0.386253\tvalid_0's l2: 0.282831\n",
      "[289]\ttraining's l1: 0.247777\ttraining's l2: 0.148822\tvalid_0's l1: 0.386195\tvalid_0's l2: 0.282753\n",
      "[290]\ttraining's l1: 0.247499\ttraining's l2: 0.148443\tvalid_0's l1: 0.386116\tvalid_0's l2: 0.282586\n",
      "[291]\ttraining's l1: 0.247249\ttraining's l2: 0.148124\tvalid_0's l1: 0.386243\tvalid_0's l2: 0.282711\n",
      "[292]\ttraining's l1: 0.246932\ttraining's l2: 0.147787\tvalid_0's l1: 0.386373\tvalid_0's l2: 0.282726\n",
      "[293]\ttraining's l1: 0.246665\ttraining's l2: 0.147367\tvalid_0's l1: 0.386631\tvalid_0's l2: 0.282964\n",
      "[294]\ttraining's l1: 0.246302\ttraining's l2: 0.146985\tvalid_0's l1: 0.386642\tvalid_0's l2: 0.283079\n",
      "[295]\ttraining's l1: 0.245983\ttraining's l2: 0.146586\tvalid_0's l1: 0.386614\tvalid_0's l2: 0.283178\n",
      "[296]\ttraining's l1: 0.245682\ttraining's l2: 0.146259\tvalid_0's l1: 0.386742\tvalid_0's l2: 0.283196\n",
      "[297]\ttraining's l1: 0.245392\ttraining's l2: 0.145903\tvalid_0's l1: 0.386783\tvalid_0's l2: 0.283226\n",
      "[298]\ttraining's l1: 0.245007\ttraining's l2: 0.145425\tvalid_0's l1: 0.386996\tvalid_0's l2: 0.283502\n",
      "[299]\ttraining's l1: 0.244733\ttraining's l2: 0.145023\tvalid_0's l1: 0.387338\tvalid_0's l2: 0.283777\n",
      "[300]\ttraining's l1: 0.244467\ttraining's l2: 0.14471\tvalid_0's l1: 0.387324\tvalid_0's l2: 0.283752\n",
      "[301]\ttraining's l1: 0.244152\ttraining's l2: 0.144322\tvalid_0's l1: 0.387295\tvalid_0's l2: 0.28385\n",
      "[302]\ttraining's l1: 0.243731\ttraining's l2: 0.143839\tvalid_0's l1: 0.387388\tvalid_0's l2: 0.283871\n",
      "[303]\ttraining's l1: 0.243446\ttraining's l2: 0.143523\tvalid_0's l1: 0.387406\tvalid_0's l2: 0.283909\n",
      "[304]\ttraining's l1: 0.243147\ttraining's l2: 0.143144\tvalid_0's l1: 0.387427\tvalid_0's l2: 0.283852\n",
      "[305]\ttraining's l1: 0.242897\ttraining's l2: 0.142795\tvalid_0's l1: 0.387448\tvalid_0's l2: 0.283803\n",
      "[306]\ttraining's l1: 0.24253\ttraining's l2: 0.142335\tvalid_0's l1: 0.38765\tvalid_0's l2: 0.284075\n",
      "[307]\ttraining's l1: 0.242262\ttraining's l2: 0.142035\tvalid_0's l1: 0.387636\tvalid_0's l2: 0.284053\n",
      "[308]\ttraining's l1: 0.241962\ttraining's l2: 0.141642\tvalid_0's l1: 0.387539\tvalid_0's l2: 0.283993\n",
      "[309]\ttraining's l1: 0.241707\ttraining's l2: 0.14133\tvalid_0's l1: 0.387682\tvalid_0's l2: 0.284049\n",
      "[310]\ttraining's l1: 0.241279\ttraining's l2: 0.140863\tvalid_0's l1: 0.38781\tvalid_0's l2: 0.284077\n",
      "[311]\ttraining's l1: 0.241033\ttraining's l2: 0.140516\tvalid_0's l1: 0.387782\tvalid_0's l2: 0.283975\n",
      "[312]\ttraining's l1: 0.240744\ttraining's l2: 0.140148\tvalid_0's l1: 0.387784\tvalid_0's l2: 0.284077\n",
      "[313]\ttraining's l1: 0.240531\ttraining's l2: 0.139815\tvalid_0's l1: 0.387862\tvalid_0's l2: 0.284065\n",
      "[314]\ttraining's l1: 0.240184\ttraining's l2: 0.139372\tvalid_0's l1: 0.388069\tvalid_0's l2: 0.284336\n",
      "[315]\ttraining's l1: 0.23992\ttraining's l2: 0.138996\tvalid_0's l1: 0.388377\tvalid_0's l2: 0.284592\n",
      "[316]\ttraining's l1: 0.239671\ttraining's l2: 0.138707\tvalid_0's l1: 0.388404\tvalid_0's l2: 0.284572\n",
      "[317]\ttraining's l1: 0.239248\ttraining's l2: 0.138256\tvalid_0's l1: 0.388517\tvalid_0's l2: 0.284475\n",
      "[318]\ttraining's l1: 0.238935\ttraining's l2: 0.137931\tvalid_0's l1: 0.388384\tvalid_0's l2: 0.284255\n",
      "[319]\ttraining's l1: 0.238681\ttraining's l2: 0.137606\tvalid_0's l1: 0.388425\tvalid_0's l2: 0.284307\n",
      "[320]\ttraining's l1: 0.238391\ttraining's l2: 0.137244\tvalid_0's l1: 0.388417\tvalid_0's l2: 0.284377\n",
      "[321]\ttraining's l1: 0.238131\ttraining's l2: 0.136984\tvalid_0's l1: 0.388636\tvalid_0's l2: 0.284563\n",
      "[322]\ttraining's l1: 0.237733\ttraining's l2: 0.136544\tvalid_0's l1: 0.388766\tvalid_0's l2: 0.284601\n",
      "[323]\ttraining's l1: 0.23743\ttraining's l2: 0.136251\tvalid_0's l1: 0.388813\tvalid_0's l2: 0.284674\n",
      "[324]\ttraining's l1: 0.237105\ttraining's l2: 0.135827\tvalid_0's l1: 0.388982\tvalid_0's l2: 0.284928\n",
      "[325]\ttraining's l1: 0.236802\ttraining's l2: 0.135515\tvalid_0's l1: 0.388852\tvalid_0's l2: 0.284717\n",
      "[326]\ttraining's l1: 0.236561\ttraining's l2: 0.135189\tvalid_0's l1: 0.388827\tvalid_0's l2: 0.28463\n",
      "[327]\ttraining's l1: 0.236303\ttraining's l2: 0.134912\tvalid_0's l1: 0.388868\tvalid_0's l2: 0.284618\n",
      "[328]\ttraining's l1: 0.235886\ttraining's l2: 0.134486\tvalid_0's l1: 0.388976\tvalid_0's l2: 0.284535\n",
      "[329]\ttraining's l1: 0.235607\ttraining's l2: 0.134182\tvalid_0's l1: 0.388847\tvalid_0's l2: 0.284333\n",
      "[330]\ttraining's l1: 0.23536\ttraining's l2: 0.133937\tvalid_0's l1: 0.389043\tvalid_0's l2: 0.284398\n",
      "[331]\ttraining's l1: 0.235017\ttraining's l2: 0.133527\tvalid_0's l1: 0.389177\tvalid_0's l2: 0.2847\n",
      "[332]\ttraining's l1: 0.234782\ttraining's l2: 0.133259\tvalid_0's l1: 0.389218\tvalid_0's l2: 0.284693\n",
      "[333]\ttraining's l1: 0.234392\ttraining's l2: 0.132839\tvalid_0's l1: 0.389306\tvalid_0's l2: 0.284733\n",
      "[334]\ttraining's l1: 0.234095\ttraining's l2: 0.132454\tvalid_0's l1: 0.389208\tvalid_0's l2: 0.284724\n",
      "[335]\ttraining's l1: 0.233852\ttraining's l2: 0.132216\tvalid_0's l1: 0.389401\tvalid_0's l2: 0.284791\n",
      "[336]\ttraining's l1: 0.233604\ttraining's l2: 0.131902\tvalid_0's l1: 0.389503\tvalid_0's l2: 0.284824\n",
      "[337]\ttraining's l1: 0.233207\ttraining's l2: 0.131497\tvalid_0's l1: 0.389608\tvalid_0's l2: 0.284753\n",
      "[338]\ttraining's l1: 0.232963\ttraining's l2: 0.131187\tvalid_0's l1: 0.3897\tvalid_0's l2: 0.284849\n",
      "[339]\ttraining's l1: 0.232602\ttraining's l2: 0.130746\tvalid_0's l1: 0.38996\tvalid_0's l2: 0.284952\n",
      "[340]\ttraining's l1: 0.232356\ttraining's l2: 0.130464\tvalid_0's l1: 0.389929\tvalid_0's l2: 0.284963\n",
      "[341]\ttraining's l1: 0.232112\ttraining's l2: 0.130165\tvalid_0's l1: 0.389967\tvalid_0's l2: 0.285021\n",
      "[342]\ttraining's l1: 0.231776\ttraining's l2: 0.129714\tvalid_0's l1: 0.389907\tvalid_0's l2: 0.285014\n",
      "[343]\ttraining's l1: 0.231548\ttraining's l2: 0.129397\tvalid_0's l1: 0.389829\tvalid_0's l2: 0.284845\n",
      "[344]\ttraining's l1: 0.231276\ttraining's l2: 0.129123\tvalid_0's l1: 0.389881\tvalid_0's l2: 0.284905\n",
      "[345]\ttraining's l1: 0.231043\ttraining's l2: 0.128825\tvalid_0's l1: 0.389976\tvalid_0's l2: 0.284932\n",
      "[346]\ttraining's l1: 0.230676\ttraining's l2: 0.128429\tvalid_0's l1: 0.390062\tvalid_0's l2: 0.284981\n",
      "[347]\ttraining's l1: 0.230447\ttraining's l2: 0.128136\tvalid_0's l1: 0.390156\tvalid_0's l2: 0.28501\n",
      "[348]\ttraining's l1: 0.230232\ttraining's l2: 0.127871\tvalid_0's l1: 0.390164\tvalid_0's l2: 0.285019\n",
      "[349]\ttraining's l1: 0.229953\ttraining's l2: 0.127607\tvalid_0's l1: 0.390151\tvalid_0's l2: 0.285018\n",
      "[350]\ttraining's l1: 0.229637\ttraining's l2: 0.127172\tvalid_0's l1: 0.390069\tvalid_0's l2: 0.284998\n",
      "[351]\ttraining's l1: 0.229371\ttraining's l2: 0.126885\tvalid_0's l1: 0.39006\tvalid_0's l2: 0.285022\n",
      "[352]\ttraining's l1: 0.229153\ttraining's l2: 0.126643\tvalid_0's l1: 0.390098\tvalid_0's l2: 0.285023\n",
      "[353]\ttraining's l1: 0.228897\ttraining's l2: 0.12628\tvalid_0's l1: 0.390002\tvalid_0's l2: 0.285017\n",
      "[354]\ttraining's l1: 0.228481\ttraining's l2: 0.125893\tvalid_0's l1: 0.390151\tvalid_0's l2: 0.284963\n",
      "[355]\ttraining's l1: 0.228265\ttraining's l2: 0.125672\tvalid_0's l1: 0.390333\tvalid_0's l2: 0.28503\n",
      "[356]\ttraining's l1: 0.22802\ttraining's l2: 0.125356\tvalid_0's l1: 0.390284\tvalid_0's l2: 0.284943\n",
      "[357]\ttraining's l1: 0.227762\ttraining's l2: 0.125097\tvalid_0's l1: 0.390304\tvalid_0's l2: 0.284988\n",
      "[358]\ttraining's l1: 0.227486\ttraining's l2: 0.124815\tvalid_0's l1: 0.390328\tvalid_0's l2: 0.284957\n",
      "[359]\ttraining's l1: 0.22713\ttraining's l2: 0.124447\tvalid_0's l1: 0.390477\tvalid_0's l2: 0.285115\n",
      "[360]\ttraining's l1: 0.226789\ttraining's l2: 0.124123\tvalid_0's l1: 0.390615\tvalid_0's l2: 0.285283\n",
      "[361]\ttraining's l1: 0.226514\ttraining's l2: 0.123847\tvalid_0's l1: 0.390613\tvalid_0's l2: 0.285257\n",
      "[362]\ttraining's l1: 0.226268\ttraining's l2: 0.123499\tvalid_0's l1: 0.390518\tvalid_0's l2: 0.285256\n",
      "[363]\ttraining's l1: 0.226038\ttraining's l2: 0.123229\tvalid_0's l1: 0.390535\tvalid_0's l2: 0.285286\n",
      "[364]\ttraining's l1: 0.225799\ttraining's l2: 0.122984\tvalid_0's l1: 0.390522\tvalid_0's l2: 0.28529\n",
      "[365]\ttraining's l1: 0.225422\ttraining's l2: 0.122567\tvalid_0's l1: 0.390591\tvalid_0's l2: 0.285346\n",
      "[366]\ttraining's l1: 0.225096\ttraining's l2: 0.122251\tvalid_0's l1: 0.390686\tvalid_0's l2: 0.285472\n",
      "[367]\ttraining's l1: 0.224864\ttraining's l2: 0.122012\tvalid_0's l1: 0.390672\tvalid_0's l2: 0.285479\n",
      "[368]\ttraining's l1: 0.224488\ttraining's l2: 0.121645\tvalid_0's l1: 0.390811\tvalid_0's l2: 0.285435\n",
      "[369]\ttraining's l1: 0.224241\ttraining's l2: 0.121342\tvalid_0's l1: 0.390763\tvalid_0's l2: 0.285354\n",
      "[370]\ttraining's l1: 0.224054\ttraining's l2: 0.121134\tvalid_0's l1: 0.390939\tvalid_0's l2: 0.28542\n",
      "[371]\ttraining's l1: 0.223797\ttraining's l2: 0.120888\tvalid_0's l1: 0.390985\tvalid_0's l2: 0.285483\n",
      "[372]\ttraining's l1: 0.223534\ttraining's l2: 0.12063\tvalid_0's l1: 0.390959\tvalid_0's l2: 0.285433\n",
      "[373]\ttraining's l1: 0.223223\ttraining's l2: 0.120224\tvalid_0's l1: 0.390967\tvalid_0's l2: 0.285444\n",
      "[374]\ttraining's l1: 0.222905\ttraining's l2: 0.119877\tvalid_0's l1: 0.391149\tvalid_0's l2: 0.285575\n",
      "[375]\ttraining's l1: 0.222611\ttraining's l2: 0.119619\tvalid_0's l1: 0.391158\tvalid_0's l2: 0.28557\n",
      "[376]\ttraining's l1: 0.222364\ttraining's l2: 0.119289\tvalid_0's l1: 0.391048\tvalid_0's l2: 0.285552\n",
      "[377]\ttraining's l1: 0.222158\ttraining's l2: 0.119075\tvalid_0's l1: 0.390909\tvalid_0's l2: 0.285428\n",
      "[378]\ttraining's l1: 0.221971\ttraining's l2: 0.118873\tvalid_0's l1: 0.391082\tvalid_0's l2: 0.285497\n",
      "[379]\ttraining's l1: 0.22165\ttraining's l2: 0.118522\tvalid_0's l1: 0.39113\tvalid_0's l2: 0.285436\n",
      "[380]\ttraining's l1: 0.221472\ttraining's l2: 0.118325\tvalid_0's l1: 0.391301\tvalid_0's l2: 0.285507\n",
      "[381]\ttraining's l1: 0.221227\ttraining's l2: 0.118003\tvalid_0's l1: 0.391211\tvalid_0's l2: 0.285511\n",
      "[382]\ttraining's l1: 0.220962\ttraining's l2: 0.117749\tvalid_0's l1: 0.391255\tvalid_0's l2: 0.285542\n",
      "[383]\ttraining's l1: 0.220721\ttraining's l2: 0.117505\tvalid_0's l1: 0.391249\tvalid_0's l2: 0.2855\n",
      "[384]\ttraining's l1: 0.220433\ttraining's l2: 0.117162\tvalid_0's l1: 0.391317\tvalid_0's l2: 0.285615\n",
      "[385]\ttraining's l1: 0.22022\ttraining's l2: 0.116927\tvalid_0's l1: 0.391347\tvalid_0's l2: 0.285625\n",
      "[386]\ttraining's l1: 0.220029\ttraining's l2: 0.116673\tvalid_0's l1: 0.391438\tvalid_0's l2: 0.285721\n",
      "[387]\ttraining's l1: 0.219774\ttraining's l2: 0.116443\tvalid_0's l1: 0.391464\tvalid_0's l2: 0.285786\n",
      "[388]\ttraining's l1: 0.219575\ttraining's l2: 0.116238\tvalid_0's l1: 0.391336\tvalid_0's l2: 0.285668\n",
      "[389]\ttraining's l1: 0.219264\ttraining's l2: 0.115852\tvalid_0's l1: 0.391257\tvalid_0's l2: 0.285654\n",
      "[390]\ttraining's l1: 0.219007\ttraining's l2: 0.115625\tvalid_0's l1: 0.391321\tvalid_0's l2: 0.285595\n",
      "[391]\ttraining's l1: 0.218628\ttraining's l2: 0.115269\tvalid_0's l1: 0.39122\tvalid_0's l2: 0.285512\n",
      "[392]\ttraining's l1: 0.218419\ttraining's l2: 0.115044\tvalid_0's l1: 0.391283\tvalid_0's l2: 0.285555\n",
      "[393]\ttraining's l1: 0.218123\ttraining's l2: 0.114672\tvalid_0's l1: 0.391546\tvalid_0's l2: 0.285686\n",
      "[394]\ttraining's l1: 0.217955\ttraining's l2: 0.114407\tvalid_0's l1: 0.39162\tvalid_0's l2: 0.285645\n",
      "[395]\ttraining's l1: 0.217703\ttraining's l2: 0.114185\tvalid_0's l1: 0.391671\tvalid_0's l2: 0.285587\n",
      "[396]\ttraining's l1: 0.217383\ttraining's l2: 0.113804\tvalid_0's l1: 0.391718\tvalid_0's l2: 0.285616\n",
      "[397]\ttraining's l1: 0.217084\ttraining's l2: 0.113531\tvalid_0's l1: 0.391804\tvalid_0's l2: 0.285791\n",
      "[398]\ttraining's l1: 0.216846\ttraining's l2: 0.113315\tvalid_0's l1: 0.391865\tvalid_0's l2: 0.285739\n",
      "[399]\ttraining's l1: 0.21654\ttraining's l2: 0.112999\tvalid_0's l1: 0.392037\tvalid_0's l2: 0.285872\n",
      "[400]\ttraining's l1: 0.216352\ttraining's l2: 0.112806\tvalid_0's l1: 0.391913\tvalid_0's l2: 0.285763\n",
      "[401]\ttraining's l1: 0.21601\ttraining's l2: 0.112464\tvalid_0's l1: 0.391815\tvalid_0's l2: 0.285685\n",
      "[402]\ttraining's l1: 0.215766\ttraining's l2: 0.112248\tvalid_0's l1: 0.391813\tvalid_0's l2: 0.285735\n",
      "[403]\ttraining's l1: 0.215586\ttraining's l2: 0.112063\tvalid_0's l1: 0.391963\tvalid_0's l2: 0.285862\n",
      "[404]\ttraining's l1: 0.215331\ttraining's l2: 0.111842\tvalid_0's l1: 0.392013\tvalid_0's l2: 0.285916\n",
      "[405]\ttraining's l1: 0.215153\ttraining's l2: 0.111593\tvalid_0's l1: 0.392069\tvalid_0's l2: 0.285889\n",
      "[406]\ttraining's l1: 0.214875\ttraining's l2: 0.111239\tvalid_0's l1: 0.392325\tvalid_0's l2: 0.286022\n",
      "[407]\ttraining's l1: 0.214557\ttraining's l2: 0.110879\tvalid_0's l1: 0.392289\tvalid_0's l2: 0.286029\n",
      "[408]\ttraining's l1: 0.214329\ttraining's l2: 0.110671\tvalid_0's l1: 0.392349\tvalid_0's l2: 0.285981\n",
      "[409]\ttraining's l1: 0.214077\ttraining's l2: 0.110407\tvalid_0's l1: 0.392409\tvalid_0's l2: 0.286184\n",
      "[410]\ttraining's l1: 0.213876\ttraining's l2: 0.110159\tvalid_0's l1: 0.392447\tvalid_0's l2: 0.286168\n",
      "[411]\ttraining's l1: 0.213573\ttraining's l2: 0.109808\tvalid_0's l1: 0.392412\tvalid_0's l2: 0.286178\n",
      "[412]\ttraining's l1: 0.213328\ttraining's l2: 0.109596\tvalid_0's l1: 0.392494\tvalid_0's l2: 0.286181\n",
      "[413]\ttraining's l1: 0.213059\ttraining's l2: 0.109316\tvalid_0's l1: 0.392568\tvalid_0's l2: 0.286242\n",
      "[414]\ttraining's l1: 0.212835\ttraining's l2: 0.109095\tvalid_0's l1: 0.392563\tvalid_0's l2: 0.286223\n",
      "[415]\ttraining's l1: 0.212629\ttraining's l2: 0.108864\tvalid_0's l1: 0.392484\tvalid_0's l2: 0.286123\n",
      "[416]\ttraining's l1: 0.212456\ttraining's l2: 0.10863\tvalid_0's l1: 0.392552\tvalid_0's l2: 0.286094\n",
      "[417]\ttraining's l1: 0.212175\ttraining's l2: 0.108353\tvalid_0's l1: 0.392622\tvalid_0's l2: 0.286258\n",
      "[418]\ttraining's l1: 0.21196\ttraining's l2: 0.108141\tvalid_0's l1: 0.392522\tvalid_0's l2: 0.286158\n",
      "[419]\ttraining's l1: 0.211649\ttraining's l2: 0.107851\tvalid_0's l1: 0.39265\tvalid_0's l2: 0.286318\n",
      "[420]\ttraining's l1: 0.211383\ttraining's l2: 0.107557\tvalid_0's l1: 0.392553\tvalid_0's l2: 0.286241\n",
      "[421]\ttraining's l1: 0.211091\ttraining's l2: 0.107288\tvalid_0's l1: 0.392647\tvalid_0's l2: 0.286421\n",
      "[422]\ttraining's l1: 0.210911\ttraining's l2: 0.107063\tvalid_0's l1: 0.392681\tvalid_0's l2: 0.286399\n",
      "[423]\ttraining's l1: 0.210632\ttraining's l2: 0.106811\tvalid_0's l1: 0.39278\tvalid_0's l2: 0.28662\n",
      "[424]\ttraining's l1: 0.210452\ttraining's l2: 0.106618\tvalid_0's l1: 0.392646\tvalid_0's l2: 0.286404\n",
      "[425]\ttraining's l1: 0.210155\ttraining's l2: 0.106265\tvalid_0's l1: 0.392658\tvalid_0's l2: 0.286425\n",
      "[426]\ttraining's l1: 0.209893\ttraining's l2: 0.105981\tvalid_0's l1: 0.392563\tvalid_0's l2: 0.286352\n",
      "[427]\ttraining's l1: 0.209668\ttraining's l2: 0.105761\tvalid_0's l1: 0.392584\tvalid_0's l2: 0.286343\n",
      "[428]\ttraining's l1: 0.209436\ttraining's l2: 0.105561\tvalid_0's l1: 0.392695\tvalid_0's l2: 0.28644\n",
      "[429]\ttraining's l1: 0.209141\ttraining's l2: 0.105233\tvalid_0's l1: 0.392662\tvalid_0's l2: 0.286454\n",
      "[430]\ttraining's l1: 0.208935\ttraining's l2: 0.105031\tvalid_0's l1: 0.392565\tvalid_0's l2: 0.286362\n",
      "[431]\ttraining's l1: 0.208677\ttraining's l2: 0.104753\tvalid_0's l1: 0.392472\tvalid_0's l2: 0.286291\n",
      "[432]\ttraining's l1: 0.208258\ttraining's l2: 0.104401\tvalid_0's l1: 0.392442\tvalid_0's l2: 0.286116\n",
      "[433]\ttraining's l1: 0.207981\ttraining's l2: 0.10416\tvalid_0's l1: 0.392489\tvalid_0's l2: 0.286287\n",
      "[434]\ttraining's l1: 0.207811\ttraining's l2: 0.103945\tvalid_0's l1: 0.392554\tvalid_0's l2: 0.286264\n",
      "[435]\ttraining's l1: 0.207585\ttraining's l2: 0.103757\tvalid_0's l1: 0.392614\tvalid_0's l2: 0.286221\n",
      "[436]\ttraining's l1: 0.207307\ttraining's l2: 0.103521\tvalid_0's l1: 0.392679\tvalid_0's l2: 0.286372\n",
      "[437]\ttraining's l1: 0.207141\ttraining's l2: 0.103311\tvalid_0's l1: 0.392741\tvalid_0's l2: 0.286352\n",
      "[438]\ttraining's l1: 0.206861\ttraining's l2: 0.103052\tvalid_0's l1: 0.392683\tvalid_0's l2: 0.286293\n",
      "[439]\ttraining's l1: 0.206572\ttraining's l2: 0.102716\tvalid_0's l1: 0.392751\tvalid_0's l2: 0.286358\n",
      "[440]\ttraining's l1: 0.206396\ttraining's l2: 0.102549\tvalid_0's l1: 0.392637\tvalid_0's l2: 0.286269\n",
      "[441]\ttraining's l1: 0.206125\ttraining's l2: 0.102278\tvalid_0's l1: 0.392787\tvalid_0's l2: 0.286402\n",
      "[442]\ttraining's l1: 0.205952\ttraining's l2: 0.102067\tvalid_0's l1: 0.392819\tvalid_0's l2: 0.286385\n",
      "[443]\ttraining's l1: 0.205699\ttraining's l2: 0.101834\tvalid_0's l1: 0.392903\tvalid_0's l2: 0.286592\n",
      "[444]\ttraining's l1: 0.205417\ttraining's l2: 0.101537\tvalid_0's l1: 0.392852\tvalid_0's l2: 0.286667\n",
      "[445]\ttraining's l1: 0.20521\ttraining's l2: 0.101344\tvalid_0's l1: 0.392774\tvalid_0's l2: 0.286582\n",
      "[446]\ttraining's l1: 0.204777\ttraining's l2: 0.10095\tvalid_0's l1: 0.392787\tvalid_0's l2: 0.286475\n",
      "[447]\ttraining's l1: 0.204595\ttraining's l2: 0.100726\tvalid_0's l1: 0.392775\tvalid_0's l2: 0.2865\n",
      "[448]\ttraining's l1: 0.20442\ttraining's l2: 0.100552\tvalid_0's l1: 0.392939\tvalid_0's l2: 0.286646\n",
      "[449]\ttraining's l1: 0.204099\ttraining's l2: 0.1003\tvalid_0's l1: 0.393066\tvalid_0's l2: 0.286758\n",
      "[450]\ttraining's l1: 0.203905\ttraining's l2: 0.100118\tvalid_0's l1: 0.393156\tvalid_0's l2: 0.286821\n",
      "[451]\ttraining's l1: 0.203724\ttraining's l2: 0.0998982\tvalid_0's l1: 0.393154\tvalid_0's l2: 0.286846\n",
      "[452]\ttraining's l1: 0.203441\ttraining's l2: 0.0996516\tvalid_0's l1: 0.393098\tvalid_0's l2: 0.286791\n",
      "[453]\ttraining's l1: 0.203155\ttraining's l2: 0.0993756\tvalid_0's l1: 0.393221\tvalid_0's l2: 0.286773\n",
      "[454]\ttraining's l1: 0.202988\ttraining's l2: 0.099172\tvalid_0's l1: 0.393252\tvalid_0's l2: 0.286758\n",
      "[455]\ttraining's l1: 0.202791\ttraining's l2: 0.0989493\tvalid_0's l1: 0.393419\tvalid_0's l2: 0.287038\n",
      "[456]\ttraining's l1: 0.202628\ttraining's l2: 0.0987779\tvalid_0's l1: 0.393292\tvalid_0's l2: 0.286842\n",
      "[457]\ttraining's l1: 0.202442\ttraining's l2: 0.0986028\tvalid_0's l1: 0.393438\tvalid_0's l2: 0.286945\n",
      "[458]\ttraining's l1: 0.202264\ttraining's l2: 0.098388\tvalid_0's l1: 0.393437\tvalid_0's l2: 0.286972\n",
      "[459]\ttraining's l1: 0.202087\ttraining's l2: 0.0982161\tvalid_0's l1: 0.393582\tvalid_0's l2: 0.287076\n",
      "[460]\ttraining's l1: 0.201938\ttraining's l2: 0.0979979\tvalid_0's l1: 0.393779\tvalid_0's l2: 0.287323\n",
      "[461]\ttraining's l1: 0.201753\ttraining's l2: 0.0978298\tvalid_0's l1: 0.393851\tvalid_0's l2: 0.287374\n",
      "[462]\ttraining's l1: 0.201576\ttraining's l2: 0.0976189\tvalid_0's l1: 0.393849\tvalid_0's l2: 0.287402\n",
      "[463]\ttraining's l1: 0.201311\ttraining's l2: 0.0973827\tvalid_0's l1: 0.393794\tvalid_0's l2: 0.287351\n",
      "[464]\ttraining's l1: 0.201119\ttraining's l2: 0.0971691\tvalid_0's l1: 0.393953\tvalid_0's l2: 0.287622\n",
      "[465]\ttraining's l1: 0.200934\ttraining's l2: 0.0970107\tvalid_0's l1: 0.394118\tvalid_0's l2: 0.287748\n",
      "[466]\ttraining's l1: 0.200758\ttraining's l2: 0.0968036\tvalid_0's l1: 0.394116\tvalid_0's l2: 0.287777\n",
      "[467]\ttraining's l1: 0.20051\ttraining's l2: 0.0965733\tvalid_0's l1: 0.394061\tvalid_0's l2: 0.287728\n",
      "[468]\ttraining's l1: 0.200249\ttraining's l2: 0.0963241\tvalid_0's l1: 0.39415\tvalid_0's l2: 0.287827\n",
      "[469]\ttraining's l1: 0.199991\ttraining's l2: 0.0961217\tvalid_0's l1: 0.394264\tvalid_0's l2: 0.287985\n",
      "[470]\ttraining's l1: 0.199746\ttraining's l2: 0.0958961\tvalid_0's l1: 0.394209\tvalid_0's l2: 0.287939\n",
      "[471]\ttraining's l1: 0.199578\ttraining's l2: 0.09569\tvalid_0's l1: 0.39433\tvalid_0's l2: 0.288196\n",
      "[472]\ttraining's l1: 0.19936\ttraining's l2: 0.0955276\tvalid_0's l1: 0.394434\tvalid_0's l2: 0.288297\n",
      "[473]\ttraining's l1: 0.199203\ttraining's l2: 0.0953276\tvalid_0's l1: 0.394428\tvalid_0's l2: 0.288313\n",
      "[474]\ttraining's l1: 0.198924\ttraining's l2: 0.0951028\tvalid_0's l1: 0.394467\tvalid_0's l2: 0.288314\n",
      "[475]\ttraining's l1: 0.198682\ttraining's l2: 0.094883\tvalid_0's l1: 0.394412\tvalid_0's l2: 0.28827\n",
      "[476]\ttraining's l1: 0.198395\ttraining's l2: 0.0945765\tvalid_0's l1: 0.39444\tvalid_0's l2: 0.288369\n",
      "[477]\ttraining's l1: 0.198131\ttraining's l2: 0.0943818\tvalid_0's l1: 0.394586\tvalid_0's l2: 0.288536\n",
      "[478]\ttraining's l1: 0.197868\ttraining's l2: 0.0941779\tvalid_0's l1: 0.39455\tvalid_0's l2: 0.288536\n",
      "[479]\ttraining's l1: 0.197681\ttraining's l2: 0.0940238\tvalid_0's l1: 0.394515\tvalid_0's l2: 0.288516\n",
      "[480]\ttraining's l1: 0.197454\ttraining's l2: 0.0937841\tvalid_0's l1: 0.394634\tvalid_0's l2: 0.288591\n",
      "[481]\ttraining's l1: 0.197243\ttraining's l2: 0.0935186\tvalid_0's l1: 0.394703\tvalid_0's l2: 0.288691\n",
      "[482]\ttraining's l1: 0.197009\ttraining's l2: 0.0933034\tvalid_0's l1: 0.394632\tvalid_0's l2: 0.288651\n",
      "[483]\ttraining's l1: 0.196879\ttraining's l2: 0.0931073\tvalid_0's l1: 0.394819\tvalid_0's l2: 0.288893\n",
      "[484]\ttraining's l1: 0.196701\ttraining's l2: 0.092944\tvalid_0's l1: 0.394773\tvalid_0's l2: 0.288753\n",
      "[485]\ttraining's l1: 0.196432\ttraining's l2: 0.0926922\tvalid_0's l1: 0.394867\tvalid_0's l2: 0.288855\n",
      "[486]\ttraining's l1: 0.196253\ttraining's l2: 0.0925329\tvalid_0's l1: 0.394822\tvalid_0's l2: 0.288719\n",
      "[487]\ttraining's l1: 0.195995\ttraining's l2: 0.0922373\tvalid_0's l1: 0.394881\tvalid_0's l2: 0.288781\n",
      "[488]\ttraining's l1: 0.195781\ttraining's l2: 0.0920054\tvalid_0's l1: 0.394997\tvalid_0's l2: 0.288856\n",
      "[489]\ttraining's l1: 0.195596\ttraining's l2: 0.0918561\tvalid_0's l1: 0.394967\tvalid_0's l2: 0.288852\n",
      "[490]\ttraining's l1: 0.195432\ttraining's l2: 0.0916636\tvalid_0's l1: 0.394933\tvalid_0's l2: 0.288856\n",
      "[491]\ttraining's l1: 0.195191\ttraining's l2: 0.0914689\tvalid_0's l1: 0.395016\tvalid_0's l2: 0.289043\n",
      "[492]\ttraining's l1: 0.194775\ttraining's l2: 0.091113\tvalid_0's l1: 0.395052\tvalid_0's l2: 0.289045\n",
      "[493]\ttraining's l1: 0.194666\ttraining's l2: 0.0909316\tvalid_0's l1: 0.394997\tvalid_0's l2: 0.289013\n",
      "[494]\ttraining's l1: 0.194499\ttraining's l2: 0.0907846\tvalid_0's l1: 0.39512\tvalid_0's l2: 0.289102\n",
      "[495]\ttraining's l1: 0.194162\ttraining's l2: 0.090435\tvalid_0's l1: 0.395256\tvalid_0's l2: 0.289123\n",
      "[496]\ttraining's l1: 0.194\ttraining's l2: 0.0902511\tvalid_0's l1: 0.395223\tvalid_0's l2: 0.289132\n",
      "[497]\ttraining's l1: 0.19373\ttraining's l2: 0.0900106\tvalid_0's l1: 0.395027\tvalid_0's l2: 0.288999\n",
      "[498]\ttraining's l1: 0.193481\ttraining's l2: 0.0898176\tvalid_0's l1: 0.395066\tvalid_0's l2: 0.289167\n",
      "[499]\ttraining's l1: 0.193271\ttraining's l2: 0.0895653\tvalid_0's l1: 0.395134\tvalid_0's l2: 0.289269\n",
      "[500]\ttraining's l1: 0.193113\ttraining's l2: 0.0894137\tvalid_0's l1: 0.395092\tvalid_0's l2: 0.289138\n",
      "[501]\ttraining's l1: 0.192793\ttraining's l2: 0.0890737\tvalid_0's l1: 0.395226\tvalid_0's l2: 0.289161\n",
      "[502]\ttraining's l1: 0.192535\ttraining's l2: 0.0888392\tvalid_0's l1: 0.395027\tvalid_0's l2: 0.289037\n",
      "[503]\ttraining's l1: 0.192329\ttraining's l2: 0.0886193\tvalid_0's l1: 0.395109\tvalid_0's l2: 0.289139\n",
      "[504]\ttraining's l1: 0.191912\ttraining's l2: 0.0882826\tvalid_0's l1: 0.395143\tvalid_0's l2: 0.289147\n",
      "[505]\ttraining's l1: 0.19166\ttraining's l2: 0.0880496\tvalid_0's l1: 0.395009\tvalid_0's l2: 0.289057\n",
      "[506]\ttraining's l1: 0.191394\ttraining's l2: 0.087821\tvalid_0's l1: 0.395042\tvalid_0's l2: 0.289031\n",
      "[507]\ttraining's l1: 0.191187\ttraining's l2: 0.0875757\tvalid_0's l1: 0.395109\tvalid_0's l2: 0.289136\n",
      "[508]\ttraining's l1: 0.190878\ttraining's l2: 0.0873061\tvalid_0's l1: 0.395175\tvalid_0's l2: 0.289211\n",
      "[509]\ttraining's l1: 0.1907\ttraining's l2: 0.0871235\tvalid_0's l1: 0.395146\tvalid_0's l2: 0.289209\n",
      "[510]\ttraining's l1: 0.19045\ttraining's l2: 0.0868987\tvalid_0's l1: 0.394952\tvalid_0's l2: 0.289091\n",
      "[511]\ttraining's l1: 0.19019\ttraining's l2: 0.086683\tvalid_0's l1: 0.394888\tvalid_0's l2: 0.289011\n",
      "[512]\ttraining's l1: 0.190014\ttraining's l2: 0.0865046\tvalid_0's l1: 0.394859\tvalid_0's l2: 0.289011\n",
      "[513]\ttraining's l1: 0.189765\ttraining's l2: 0.0862466\tvalid_0's l1: 0.394862\tvalid_0's l2: 0.288972\n",
      "[514]\ttraining's l1: 0.189584\ttraining's l2: 0.0860668\tvalid_0's l1: 0.394928\tvalid_0's l2: 0.289003\n",
      "[515]\ttraining's l1: 0.18945\ttraining's l2: 0.0858897\tvalid_0's l1: 0.39505\tvalid_0's l2: 0.28926\n",
      "[516]\ttraining's l1: 0.189277\ttraining's l2: 0.0857523\tvalid_0's l1: 0.395023\tvalid_0's l2: 0.289257\n",
      "[517]\ttraining's l1: 0.189078\ttraining's l2: 0.0855633\tvalid_0's l1: 0.394999\tvalid_0's l2: 0.289206\n",
      "[518]\ttraining's l1: 0.188963\ttraining's l2: 0.0853968\tvalid_0's l1: 0.394948\tvalid_0's l2: 0.289183\n",
      "[519]\ttraining's l1: 0.188767\ttraining's l2: 0.0852119\tvalid_0's l1: 0.394925\tvalid_0's l2: 0.289134\n",
      "[520]\ttraining's l1: 0.188669\ttraining's l2: 0.085039\tvalid_0's l1: 0.395109\tvalid_0's l2: 0.289378\n",
      "[521]\ttraining's l1: 0.188405\ttraining's l2: 0.0848193\tvalid_0's l1: 0.394963\tvalid_0's l2: 0.289285\n",
      "[522]\ttraining's l1: 0.188056\ttraining's l2: 0.084569\tvalid_0's l1: 0.395113\tvalid_0's l2: 0.289439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[523]\ttraining's l1: 0.187794\ttraining's l2: 0.0843443\tvalid_0's l1: 0.395202\tvalid_0's l2: 0.28955\n",
      "[524]\ttraining's l1: 0.187584\ttraining's l2: 0.084166\tvalid_0's l1: 0.395185\tvalid_0's l2: 0.289528\n",
      "[525]\ttraining's l1: 0.187387\ttraining's l2: 0.0839634\tvalid_0's l1: 0.395262\tvalid_0's l2: 0.28963\n",
      "[526]\ttraining's l1: 0.186996\ttraining's l2: 0.0836527\tvalid_0's l1: 0.395285\tvalid_0's l2: 0.289551\n",
      "[527]\ttraining's l1: 0.186894\ttraining's l2: 0.0834908\tvalid_0's l1: 0.395211\tvalid_0's l2: 0.289513\n",
      "[528]\ttraining's l1: 0.186653\ttraining's l2: 0.083286\tvalid_0's l1: 0.395077\tvalid_0's l2: 0.289486\n",
      "[529]\ttraining's l1: 0.18642\ttraining's l2: 0.0831106\tvalid_0's l1: 0.395202\tvalid_0's l2: 0.289668\n",
      "[530]\ttraining's l1: 0.186186\ttraining's l2: 0.0828987\tvalid_0's l1: 0.395072\tvalid_0's l2: 0.289597\n",
      "[531]\ttraining's l1: 0.185903\ttraining's l2: 0.0826506\tvalid_0's l1: 0.395134\tvalid_0's l2: 0.289673\n",
      "[532]\ttraining's l1: 0.185597\ttraining's l2: 0.0824113\tvalid_0's l1: 0.395272\tvalid_0's l2: 0.289821\n",
      "[533]\ttraining's l1: 0.185342\ttraining's l2: 0.0822023\tvalid_0's l1: 0.3953\tvalid_0's l2: 0.289798\n",
      "[534]\ttraining's l1: 0.184965\ttraining's l2: 0.0819056\tvalid_0's l1: 0.395323\tvalid_0's l2: 0.289726\n",
      "[535]\ttraining's l1: 0.184745\ttraining's l2: 0.0817358\tvalid_0's l1: 0.395446\tvalid_0's l2: 0.289906\n",
      "[536]\ttraining's l1: 0.18442\ttraining's l2: 0.0815292\tvalid_0's l1: 0.395451\tvalid_0's l2: 0.289805\n",
      "[537]\ttraining's l1: 0.184102\ttraining's l2: 0.0812303\tvalid_0's l1: 0.395575\tvalid_0's l2: 0.289831\n",
      "[538]\ttraining's l1: 0.18385\ttraining's l2: 0.0810307\tvalid_0's l1: 0.395514\tvalid_0's l2: 0.289758\n",
      "[539]\ttraining's l1: 0.183742\ttraining's l2: 0.0808743\tvalid_0's l1: 0.395403\tvalid_0's l2: 0.289685\n",
      "[540]\ttraining's l1: 0.183503\ttraining's l2: 0.0807081\tvalid_0's l1: 0.395482\tvalid_0's l2: 0.289743\n",
      "[541]\ttraining's l1: 0.183267\ttraining's l2: 0.0805067\tvalid_0's l1: 0.395307\tvalid_0's l2: 0.289634\n",
      "[542]\ttraining's l1: 0.183019\ttraining's l2: 0.0803114\tvalid_0's l1: 0.395246\tvalid_0's l2: 0.289564\n",
      "[543]\ttraining's l1: 0.18264\ttraining's l2: 0.0800218\tvalid_0's l1: 0.395278\tvalid_0's l2: 0.289585\n",
      "[544]\ttraining's l1: 0.182537\ttraining's l2: 0.0798694\tvalid_0's l1: 0.395188\tvalid_0's l2: 0.28953\n",
      "[545]\ttraining's l1: 0.182301\ttraining's l2: 0.0797071\tvalid_0's l1: 0.395267\tvalid_0's l2: 0.289589\n",
      "[546]\ttraining's l1: 0.182074\ttraining's l2: 0.0795481\tvalid_0's l1: 0.395361\tvalid_0's l2: 0.289648\n",
      "[547]\ttraining's l1: 0.181875\ttraining's l2: 0.0793753\tvalid_0's l1: 0.395345\tvalid_0's l2: 0.289646\n",
      "[548]\ttraining's l1: 0.181628\ttraining's l2: 0.0791847\tvalid_0's l1: 0.395265\tvalid_0's l2: 0.289579\n",
      "[549]\ttraining's l1: 0.181377\ttraining's l2: 0.0789932\tvalid_0's l1: 0.39533\tvalid_0's l2: 0.289663\n",
      "[550]\ttraining's l1: 0.18119\ttraining's l2: 0.0788254\tvalid_0's l1: 0.395257\tvalid_0's l2: 0.289624\n",
      "[551]\ttraining's l1: 0.180949\ttraining's l2: 0.078639\tvalid_0's l1: 0.395198\tvalid_0's l2: 0.289559\n",
      "[552]\ttraining's l1: 0.180844\ttraining's l2: 0.0784908\tvalid_0's l1: 0.39509\tvalid_0's l2: 0.289491\n",
      "[553]\ttraining's l1: 0.180615\ttraining's l2: 0.0783365\tvalid_0's l1: 0.395179\tvalid_0's l2: 0.289552\n",
      "[554]\ttraining's l1: 0.180476\ttraining's l2: 0.0781782\tvalid_0's l1: 0.395324\tvalid_0's l2: 0.289798\n",
      "[555]\ttraining's l1: 0.180293\ttraining's l2: 0.0780165\tvalid_0's l1: 0.395285\tvalid_0's l2: 0.289757\n",
      "[556]\ttraining's l1: 0.179994\ttraining's l2: 0.0777639\tvalid_0's l1: 0.395388\tvalid_0's l2: 0.289928\n",
      "[557]\ttraining's l1: 0.179701\ttraining's l2: 0.0775425\tvalid_0's l1: 0.395486\tvalid_0's l2: 0.290069\n",
      "[558]\ttraining's l1: 0.179451\ttraining's l2: 0.0773559\tvalid_0's l1: 0.395369\tvalid_0's l2: 0.290049\n",
      "[559]\ttraining's l1: 0.179265\ttraining's l2: 0.0772005\tvalid_0's l1: 0.395422\tvalid_0's l2: 0.290228\n",
      "[560]\ttraining's l1: 0.17917\ttraining's l2: 0.0770558\tvalid_0's l1: 0.395332\tvalid_0's l2: 0.290176\n",
      "[561]\ttraining's l1: 0.178951\ttraining's l2: 0.0769067\tvalid_0's l1: 0.395427\tvalid_0's l2: 0.290239\n",
      "[562]\ttraining's l1: 0.178602\ttraining's l2: 0.0766403\tvalid_0's l1: 0.395415\tvalid_0's l2: 0.290199\n",
      "[563]\ttraining's l1: 0.17842\ttraining's l2: 0.0764778\tvalid_0's l1: 0.395403\tvalid_0's l2: 0.2902\n",
      "[564]\ttraining's l1: 0.178222\ttraining's l2: 0.0763266\tvalid_0's l1: 0.395464\tvalid_0's l2: 0.290377\n",
      "[565]\ttraining's l1: 0.178057\ttraining's l2: 0.0761599\tvalid_0's l1: 0.395652\tvalid_0's l2: 0.290644\n",
      "[566]\ttraining's l1: 0.177744\ttraining's l2: 0.0759633\tvalid_0's l1: 0.395678\tvalid_0's l2: 0.290566\n",
      "[567]\ttraining's l1: 0.177477\ttraining's l2: 0.0757361\tvalid_0's l1: 0.395794\tvalid_0's l2: 0.290676\n",
      "[568]\ttraining's l1: 0.177175\ttraining's l2: 0.075495\tvalid_0's l1: 0.395893\tvalid_0's l2: 0.290847\n",
      "[569]\ttraining's l1: 0.176939\ttraining's l2: 0.0753186\tvalid_0's l1: 0.395862\tvalid_0's l2: 0.290786\n",
      "[570]\ttraining's l1: 0.176838\ttraining's l2: 0.0751781\tvalid_0's l1: 0.395774\tvalid_0's l2: 0.290722\n",
      "[571]\ttraining's l1: 0.176614\ttraining's l2: 0.0750311\tvalid_0's l1: 0.395874\tvalid_0's l2: 0.290791\n",
      "[572]\ttraining's l1: 0.17645\ttraining's l2: 0.0748674\tvalid_0's l1: 0.396051\tvalid_0's l2: 0.291067\n",
      "[573]\ttraining's l1: 0.176257\ttraining's l2: 0.0747169\tvalid_0's l1: 0.39604\tvalid_0's l2: 0.291052\n",
      "[574]\ttraining's l1: 0.175965\ttraining's l2: 0.0745084\tvalid_0's l1: 0.396145\tvalid_0's l2: 0.291192\n",
      "[575]\ttraining's l1: 0.175678\ttraining's l2: 0.0742745\tvalid_0's l1: 0.396288\tvalid_0's l2: 0.291363\n",
      "[576]\ttraining's l1: 0.175569\ttraining's l2: 0.0741383\tvalid_0's l1: 0.39626\tvalid_0's l2: 0.291334\n",
      "[577]\ttraining's l1: 0.175343\ttraining's l2: 0.0739606\tvalid_0's l1: 0.396146\tvalid_0's l2: 0.291315\n",
      "[578]\ttraining's l1: 0.175127\ttraining's l2: 0.0737902\tvalid_0's l1: 0.396154\tvalid_0's l2: 0.291306\n",
      "[579]\ttraining's l1: 0.174903\ttraining's l2: 0.073616\tvalid_0's l1: 0.396044\tvalid_0's l2: 0.291289\n",
      "[580]\ttraining's l1: 0.174691\ttraining's l2: 0.0734706\tvalid_0's l1: 0.39618\tvalid_0's l2: 0.291483\n",
      "[581]\ttraining's l1: 0.174366\ttraining's l2: 0.0732208\tvalid_0's l1: 0.396187\tvalid_0's l2: 0.291449\n",
      "[582]\ttraining's l1: 0.174186\ttraining's l2: 0.0730686\tvalid_0's l1: 0.396188\tvalid_0's l2: 0.291453\n",
      "[583]\ttraining's l1: 0.173975\ttraining's l2: 0.072902\tvalid_0's l1: 0.396211\tvalid_0's l2: 0.291445\n",
      "[584]\ttraining's l1: 0.173731\ttraining's l2: 0.0727041\tvalid_0's l1: 0.3964\tvalid_0's l2: 0.291557\n",
      "[585]\ttraining's l1: 0.173603\ttraining's l2: 0.072548\tvalid_0's l1: 0.396599\tvalid_0's l2: 0.291812\n",
      "[586]\ttraining's l1: 0.173327\ttraining's l2: 0.0723231\tvalid_0's l1: 0.396747\tvalid_0's l2: 0.291982\n",
      "[587]\ttraining's l1: 0.173139\ttraining's l2: 0.0721769\tvalid_0's l1: 0.396674\tvalid_0's l2: 0.291929\n",
      "[588]\ttraining's l1: 0.17289\ttraining's l2: 0.0719907\tvalid_0's l1: 0.396767\tvalid_0's l2: 0.292012\n",
      "[589]\ttraining's l1: 0.172747\ttraining's l2: 0.0718559\tvalid_0's l1: 0.396774\tvalid_0's l2: 0.292005\n",
      "[590]\ttraining's l1: 0.17252\ttraining's l2: 0.0717183\tvalid_0's l1: 0.396864\tvalid_0's l2: 0.292075\n",
      "[591]\ttraining's l1: 0.172298\ttraining's l2: 0.0714622\tvalid_0's l1: 0.396973\tvalid_0's l2: 0.292147\n",
      "[592]\ttraining's l1: 0.172138\ttraining's l2: 0.0713199\tvalid_0's l1: 0.397115\tvalid_0's l2: 0.292295\n",
      "[593]\ttraining's l1: 0.171988\ttraining's l2: 0.0711705\tvalid_0's l1: 0.397276\tvalid_0's l2: 0.292553\n",
      "[594]\ttraining's l1: 0.17173\ttraining's l2: 0.0709974\tvalid_0's l1: 0.397136\tvalid_0's l2: 0.29248\n",
      "[595]\ttraining's l1: 0.171521\ttraining's l2: 0.0708305\tvalid_0's l1: 0.397217\tvalid_0's l2: 0.292563\n",
      "[596]\ttraining's l1: 0.171341\ttraining's l2: 0.0706711\tvalid_0's l1: 0.397253\tvalid_0's l2: 0.292538\n",
      "[597]\ttraining's l1: 0.171213\ttraining's l2: 0.0705405\tvalid_0's l1: 0.39723\tvalid_0's l2: 0.292495\n",
      "[598]\ttraining's l1: 0.171025\ttraining's l2: 0.0704098\tvalid_0's l1: 0.397279\tvalid_0's l2: 0.29258\n",
      "[599]\ttraining's l1: 0.170802\ttraining's l2: 0.0702358\tvalid_0's l1: 0.397104\tvalid_0's l2: 0.292473\n",
      "[600]\ttraining's l1: 0.170535\ttraining's l2: 0.0700212\tvalid_0's l1: 0.397276\tvalid_0's l2: 0.292641\n",
      "[601]\ttraining's l1: 0.170306\ttraining's l2: 0.0697947\tvalid_0's l1: 0.39731\tvalid_0's l2: 0.292779\n",
      "[602]\ttraining's l1: 0.17009\ttraining's l2: 0.0695454\tvalid_0's l1: 0.397387\tvalid_0's l2: 0.292841\n",
      "[603]\ttraining's l1: 0.169908\ttraining's l2: 0.0693909\tvalid_0's l1: 0.39743\tvalid_0's l2: 0.292837\n",
      "[604]\ttraining's l1: 0.169648\ttraining's l2: 0.0692304\tvalid_0's l1: 0.397475\tvalid_0's l2: 0.292818\n",
      "[605]\ttraining's l1: 0.169432\ttraining's l2: 0.0690101\tvalid_0's l1: 0.397508\tvalid_0's l2: 0.292956\n",
      "[606]\ttraining's l1: 0.169212\ttraining's l2: 0.0688271\tvalid_0's l1: 0.397684\tvalid_0's l2: 0.293068\n",
      "[607]\ttraining's l1: 0.168944\ttraining's l2: 0.0686302\tvalid_0's l1: 0.397777\tvalid_0's l2: 0.293186\n",
      "[608]\ttraining's l1: 0.168632\ttraining's l2: 0.0684035\tvalid_0's l1: 0.397783\tvalid_0's l2: 0.293159\n",
      "[609]\ttraining's l1: 0.16842\ttraining's l2: 0.0682244\tvalid_0's l1: 0.397957\tvalid_0's l2: 0.293271\n",
      "[610]\ttraining's l1: 0.168301\ttraining's l2: 0.0680826\tvalid_0's l1: 0.398143\tvalid_0's l2: 0.293518\n",
      "[611]\ttraining's l1: 0.168044\ttraining's l2: 0.0678779\tvalid_0's l1: 0.39831\tvalid_0's l2: 0.293684\n",
      "[612]\ttraining's l1: 0.16781\ttraining's l2: 0.0677227\tvalid_0's l1: 0.398321\tvalid_0's l2: 0.293643\n",
      "[613]\ttraining's l1: 0.167614\ttraining's l2: 0.0674858\tvalid_0's l1: 0.398404\tvalid_0's l2: 0.293703\n",
      "[614]\ttraining's l1: 0.167455\ttraining's l2: 0.0673405\tvalid_0's l1: 0.398354\tvalid_0's l2: 0.293626\n",
      "[615]\ttraining's l1: 0.167312\ttraining's l2: 0.0672025\tvalid_0's l1: 0.398518\tvalid_0's l2: 0.293884\n",
      "[616]\ttraining's l1: 0.167108\ttraining's l2: 0.067038\tvalid_0's l1: 0.398537\tvalid_0's l2: 0.293845\n",
      "[617]\ttraining's l1: 0.166962\ttraining's l2: 0.066912\tvalid_0's l1: 0.398545\tvalid_0's l2: 0.293812\n",
      "[618]\ttraining's l1: 0.166815\ttraining's l2: 0.0667873\tvalid_0's l1: 0.398573\tvalid_0's l2: 0.293807\n",
      "[619]\ttraining's l1: 0.166573\ttraining's l2: 0.06659\tvalid_0's l1: 0.39875\tvalid_0's l2: 0.293971\n",
      "[620]\ttraining's l1: 0.166418\ttraining's l2: 0.0664491\tvalid_0's l1: 0.398713\tvalid_0's l2: 0.293897\n",
      "[621]\ttraining's l1: 0.16624\ttraining's l2: 0.0663272\tvalid_0's l1: 0.398783\tvalid_0's l2: 0.294032\n",
      "[622]\ttraining's l1: 0.165965\ttraining's l2: 0.0661329\tvalid_0's l1: 0.398815\tvalid_0's l2: 0.294063\n",
      "[623]\ttraining's l1: 0.165701\ttraining's l2: 0.065947\tvalid_0's l1: 0.398928\tvalid_0's l2: 0.294188\n",
      "[624]\ttraining's l1: 0.165485\ttraining's l2: 0.0658018\tvalid_0's l1: 0.398987\tvalid_0's l2: 0.294263\n",
      "[625]\ttraining's l1: 0.16531\ttraining's l2: 0.0656722\tvalid_0's l1: 0.398924\tvalid_0's l2: 0.294221\n",
      "[626]\ttraining's l1: 0.165051\ttraining's l2: 0.0654905\tvalid_0's l1: 0.399034\tvalid_0's l2: 0.294346\n",
      "[627]\ttraining's l1: 0.164854\ttraining's l2: 0.0653338\tvalid_0's l1: 0.399048\tvalid_0's l2: 0.294301\n",
      "[628]\ttraining's l1: 0.164752\ttraining's l2: 0.0652099\tvalid_0's l1: 0.399049\tvalid_0's l2: 0.294208\n",
      "[629]\ttraining's l1: 0.164571\ttraining's l2: 0.0650418\tvalid_0's l1: 0.399196\tvalid_0's l2: 0.294287\n",
      "[630]\ttraining's l1: 0.16438\ttraining's l2: 0.0648876\tvalid_0's l1: 0.399225\tvalid_0's l2: 0.294255\n",
      "[631]\ttraining's l1: 0.164166\ttraining's l2: 0.0647477\tvalid_0's l1: 0.399306\tvalid_0's l2: 0.294339\n",
      "[632]\ttraining's l1: 0.163877\ttraining's l2: 0.0645387\tvalid_0's l1: 0.399326\tvalid_0's l2: 0.294329\n",
      "[633]\ttraining's l1: 0.16372\ttraining's l2: 0.0644185\tvalid_0's l1: 0.399357\tvalid_0's l2: 0.294326\n",
      "[634]\ttraining's l1: 0.16349\ttraining's l2: 0.0642464\tvalid_0's l1: 0.39953\tvalid_0's l2: 0.294469\n",
      "[635]\ttraining's l1: 0.163315\ttraining's l2: 0.0641292\tvalid_0's l1: 0.399598\tvalid_0's l2: 0.294604\n",
      "[636]\ttraining's l1: 0.163089\ttraining's l2: 0.0639419\tvalid_0's l1: 0.39978\tvalid_0's l2: 0.294786\n",
      "[637]\ttraining's l1: 0.162812\ttraining's l2: 0.0637435\tvalid_0's l1: 0.399824\tvalid_0's l2: 0.294711\n",
      "[638]\ttraining's l1: 0.162639\ttraining's l2: 0.0636287\tvalid_0's l1: 0.399898\tvalid_0's l2: 0.294845\n",
      "[639]\ttraining's l1: 0.162391\ttraining's l2: 0.0634711\tvalid_0's l1: 0.399751\tvalid_0's l2: 0.294753\n",
      "[640]\ttraining's l1: 0.162178\ttraining's l2: 0.0632746\tvalid_0's l1: 0.399792\tvalid_0's l2: 0.294898\n",
      "[641]\ttraining's l1: 0.161936\ttraining's l2: 0.0631188\tvalid_0's l1: 0.399778\tvalid_0's l2: 0.294781\n",
      "[642]\ttraining's l1: 0.161681\ttraining's l2: 0.062945\tvalid_0's l1: 0.399886\tvalid_0's l2: 0.294895\n",
      "[643]\ttraining's l1: 0.161512\ttraining's l2: 0.0628095\tvalid_0's l1: 0.399971\tvalid_0's l2: 0.29495\n",
      "[644]\ttraining's l1: 0.161358\ttraining's l2: 0.0626936\tvalid_0's l1: 0.400008\tvalid_0's l2: 0.294948\n",
      "[645]\ttraining's l1: 0.161176\ttraining's l2: 0.062535\tvalid_0's l1: 0.400153\tvalid_0's l2: 0.295028\n",
      "[646]\ttraining's l1: 0.161043\ttraining's l2: 0.0624223\tvalid_0's l1: 0.400172\tvalid_0's l2: 0.294965\n",
      "[647]\ttraining's l1: 0.160792\ttraining's l2: 0.0622519\tvalid_0's l1: 0.400279\tvalid_0's l2: 0.295087\n",
      "[648]\ttraining's l1: 0.160626\ttraining's l2: 0.0621404\tvalid_0's l1: 0.400354\tvalid_0's l2: 0.295221\n",
      "[649]\ttraining's l1: 0.160415\ttraining's l2: 0.0619776\tvalid_0's l1: 0.400444\tvalid_0's l2: 0.29532\n",
      "[650]\ttraining's l1: 0.16017\ttraining's l2: 0.0618388\tvalid_0's l1: 0.400494\tvalid_0's l2: 0.295311\n",
      "[651]\ttraining's l1: 0.159908\ttraining's l2: 0.0616447\tvalid_0's l1: 0.400502\tvalid_0's l2: 0.295306\n",
      "[652]\ttraining's l1: 0.159705\ttraining's l2: 0.0614864\tvalid_0's l1: 0.400585\tvalid_0's l2: 0.295404\n",
      "[653]\ttraining's l1: 0.159523\ttraining's l2: 0.0613672\tvalid_0's l1: 0.400487\tvalid_0's l2: 0.29534\n",
      "[654]\ttraining's l1: 0.159302\ttraining's l2: 0.0611575\tvalid_0's l1: 0.400563\tvalid_0's l2: 0.295446\n",
      "[655]\ttraining's l1: 0.159104\ttraining's l2: 0.060974\tvalid_0's l1: 0.400607\tvalid_0's l2: 0.295566\n",
      "[656]\ttraining's l1: 0.158869\ttraining's l2: 0.06084\tvalid_0's l1: 0.400669\tvalid_0's l2: 0.29556\n",
      "[657]\ttraining's l1: 0.158636\ttraining's l2: 0.0606913\tvalid_0's l1: 0.400644\tvalid_0's l2: 0.295452\n",
      "[658]\ttraining's l1: 0.158412\ttraining's l2: 0.060537\tvalid_0's l1: 0.400667\tvalid_0's l2: 0.295468\n",
      "[659]\ttraining's l1: 0.158304\ttraining's l2: 0.0604327\tvalid_0's l1: 0.400687\tvalid_0's l2: 0.295421\n",
      "[660]\ttraining's l1: 0.158096\ttraining's l2: 0.0602747\tvalid_0's l1: 0.400842\tvalid_0's l2: 0.29556\n",
      "[661]\ttraining's l1: 0.158002\ttraining's l2: 0.0601624\tvalid_0's l1: 0.400827\tvalid_0's l2: 0.295473\n",
      "[662]\ttraining's l1: 0.157797\ttraining's l2: 0.0600079\tvalid_0's l1: 0.400978\tvalid_0's l2: 0.295613\n",
      "[663]\ttraining's l1: 0.157623\ttraining's l2: 0.0598809\tvalid_0's l1: 0.401042\tvalid_0's l2: 0.295655\n",
      "[664]\ttraining's l1: 0.157394\ttraining's l2: 0.0597177\tvalid_0's l1: 0.401091\tvalid_0's l2: 0.29581\n",
      "[665]\ttraining's l1: 0.157174\ttraining's l2: 0.059545\tvalid_0's l1: 0.401254\tvalid_0's l2: 0.295986\n",
      "[666]\ttraining's l1: 0.156932\ttraining's l2: 0.0593648\tvalid_0's l1: 0.401264\tvalid_0's l2: 0.295909\n",
      "[667]\ttraining's l1: 0.15678\ttraining's l2: 0.0592598\tvalid_0's l1: 0.401338\tvalid_0's l2: 0.296042\n",
      "[668]\ttraining's l1: 0.156542\ttraining's l2: 0.0590831\tvalid_0's l1: 0.401351\tvalid_0's l2: 0.295967\n",
      "[669]\ttraining's l1: 0.156319\ttraining's l2: 0.0589494\tvalid_0's l1: 0.401324\tvalid_0's l2: 0.295865\n",
      "[670]\ttraining's l1: 0.156087\ttraining's l2: 0.0587859\tvalid_0's l1: 0.40151\tvalid_0's l2: 0.296057\n",
      "[671]\ttraining's l1: 0.155883\ttraining's l2: 0.0586471\tvalid_0's l1: 0.401404\tvalid_0's l2: 0.296009\n",
      "[672]\ttraining's l1: 0.155671\ttraining's l2: 0.0584875\tvalid_0's l1: 0.401349\tvalid_0's l2: 0.295968\n",
      "[673]\ttraining's l1: 0.155453\ttraining's l2: 0.0583321\tvalid_0's l1: 0.401389\tvalid_0's l2: 0.29612\n",
      "[674]\ttraining's l1: 0.155258\ttraining's l2: 0.0581594\tvalid_0's l1: 0.401428\tvalid_0's l2: 0.29624\n",
      "[675]\ttraining's l1: 0.15505\ttraining's l2: 0.0580145\tvalid_0's l1: 0.401474\tvalid_0's l2: 0.296273\n",
      "[676]\ttraining's l1: 0.15485\ttraining's l2: 0.0578884\tvalid_0's l1: 0.401513\tvalid_0's l2: 0.296246\n",
      "[677]\ttraining's l1: 0.15463\ttraining's l2: 0.0577479\tvalid_0's l1: 0.401494\tvalid_0's l2: 0.296144\n",
      "[678]\ttraining's l1: 0.154419\ttraining's l2: 0.0576008\tvalid_0's l1: 0.40164\tvalid_0's l2: 0.296282\n",
      "[679]\ttraining's l1: 0.154131\ttraining's l2: 0.0574335\tvalid_0's l1: 0.401856\tvalid_0's l2: 0.296576\n",
      "[680]\ttraining's l1: 0.153923\ttraining's l2: 0.0572964\tvalid_0's l1: 0.401837\tvalid_0's l2: 0.296477\n",
      "[681]\ttraining's l1: 0.153819\ttraining's l2: 0.0571933\tvalid_0's l1: 0.401837\tvalid_0's l2: 0.296436\n",
      "[682]\ttraining's l1: 0.153637\ttraining's l2: 0.0570527\tvalid_0's l1: 0.401996\tvalid_0's l2: 0.296519\n",
      "[683]\ttraining's l1: 0.153418\ttraining's l2: 0.0569063\tvalid_0's l1: 0.402102\tvalid_0's l2: 0.296647\n",
      "[684]\ttraining's l1: 0.153202\ttraining's l2: 0.0567567\tvalid_0's l1: 0.402135\tvalid_0's l2: 0.296799\n",
      "[685]\ttraining's l1: 0.153055\ttraining's l2: 0.0566581\tvalid_0's l1: 0.402186\tvalid_0's l2: 0.296902\n",
      "[686]\ttraining's l1: 0.152878\ttraining's l2: 0.0565537\tvalid_0's l1: 0.402293\tvalid_0's l2: 0.297011\n",
      "[687]\ttraining's l1: 0.152622\ttraining's l2: 0.0564104\tvalid_0's l1: 0.402183\tvalid_0's l2: 0.296942\n",
      "[688]\ttraining's l1: 0.152417\ttraining's l2: 0.0562618\tvalid_0's l1: 0.402144\tvalid_0's l2: 0.296832\n",
      "[689]\ttraining's l1: 0.152208\ttraining's l2: 0.0561169\tvalid_0's l1: 0.402186\tvalid_0's l2: 0.296982\n",
      "[690]\ttraining's l1: 0.15203\ttraining's l2: 0.055999\tvalid_0's l1: 0.402203\tvalid_0's l2: 0.297012\n",
      "[691]\ttraining's l1: 0.151886\ttraining's l2: 0.0559043\tvalid_0's l1: 0.402258\tvalid_0's l2: 0.297113\n",
      "[692]\ttraining's l1: 0.151709\ttraining's l2: 0.0557886\tvalid_0's l1: 0.402277\tvalid_0's l2: 0.297143\n",
      "[693]\ttraining's l1: 0.151492\ttraining's l2: 0.0556469\tvalid_0's l1: 0.40238\tvalid_0's l2: 0.297272\n",
      "[694]\ttraining's l1: 0.151318\ttraining's l2: 0.0555468\tvalid_0's l1: 0.402479\tvalid_0's l2: 0.29738\n",
      "[695]\ttraining's l1: 0.151114\ttraining's l2: 0.055399\tvalid_0's l1: 0.402461\tvalid_0's l2: 0.297344\n",
      "[696]\ttraining's l1: 0.150956\ttraining's l2: 0.0553016\tvalid_0's l1: 0.402489\tvalid_0's l2: 0.297436\n",
      "[697]\ttraining's l1: 0.150782\ttraining's l2: 0.0551887\tvalid_0's l1: 0.402515\tvalid_0's l2: 0.297468\n",
      "[698]\ttraining's l1: 0.150617\ttraining's l2: 0.0550666\tvalid_0's l1: 0.402592\tvalid_0's l2: 0.297579\n",
      "[699]\ttraining's l1: 0.150386\ttraining's l2: 0.0549266\tvalid_0's l1: 0.402687\tvalid_0's l2: 0.297692\n",
      "[700]\ttraining's l1: 0.150108\ttraining's l2: 0.0547713\tvalid_0's l1: 0.402915\tvalid_0's l2: 0.297979\n",
      "[701]\ttraining's l1: 0.149922\ttraining's l2: 0.0546638\tvalid_0's l1: 0.40293\tvalid_0's l2: 0.298\n",
      "[702]\ttraining's l1: 0.149762\ttraining's l2: 0.0545475\tvalid_0's l1: 0.40295\tvalid_0's l2: 0.298042\n",
      "[703]\ttraining's l1: 0.14954\ttraining's l2: 0.0544134\tvalid_0's l1: 0.402971\tvalid_0's l2: 0.297945\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's l1: 0.375933\ttraining's l2: 0.394606\tvalid_0's l1: 0.365778\tvalid_0's l2: 0.34173\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    #'num_leaves': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'metric' : ['l1','l2'],\n",
    "    'random_state': 501,\n",
    "}\n",
    "lgb_train = lgb.Dataset(train_input, train_target)\n",
    "lgb_eval = lgb.Dataset(val_input, val_target)\n",
    "\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=20000,valid_sets=[lgb_eval, lgb_train],early_stopping_rounds=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbm.predict(val_input)\n",
    "target_part = np.array(train_df[train_df.columns[-1:]].iloc[-145], dtype=np.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trans=[]\n",
    "first=True\n",
    "for diff in pred:\n",
    "    if first:\n",
    "        pred_trans.append(target_part[0]+diff)\n",
    "        first=False\n",
    "    else:\n",
    "        before = pred_trans[-1]\n",
    "        pred_trans.append(before+diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Is ======================\n",
      "MSE ==\n",
      "69.44308700196952\n",
      "MAE ==\n",
      "5.89422374564548\n"
     ]
    }
   ],
   "source": [
    "real = np.array(train_df[train_df.columns[-1:]].iloc[-144:], dtype=np.float32).reshape(-1)\n",
    "print(\"Result Is ======================\")\n",
    "print(\"MSE ==\")\n",
    "print(mean_squared_error(real, pred_trans))\n",
    "print(\"MAE ==\")\n",
    "print(mean_absolute_error(real, pred_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Is ======================\n",
      "MSE ==\n",
      "24.917177842711055\n",
      "MAE ==\n",
      "3.752615207410364\n",
      "Train Result Is ======================\n",
      "MSE ==\n",
      "24.67829501834971\n",
      "MAE ==\n",
      "3.966809056711414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "clf = SVR(C=2)\n",
    "clf.fit(train_input, train_target)\n",
    "pred = clf.predict(val_input)\n",
    "target_part = np.array(train_df[train_df.columns[-1:]].iloc[-145], dtype=np.float32).reshape(-1)\n",
    "pred_trans=[]\n",
    "first=True\n",
    "for diff in pred:\n",
    "    if first:\n",
    "        pred_trans.append(target_part[0]+diff)\n",
    "        first=False\n",
    "    else:\n",
    "        before = pred_trans[-1]\n",
    "        pred_trans.append(before+diff)\n",
    "        \n",
    "real = np.array(train_df[train_df.columns[-1:]].iloc[-144:], dtype=np.float32).reshape(-1)\n",
    "print(\"Result Is ======================\")\n",
    "print(\"MSE ==\")\n",
    "print(mean_squared_error(real, pred_trans))\n",
    "print(\"MAE ==\")\n",
    "print(mean_absolute_error(real, pred_trans))\n",
    "\n",
    "\n",
    "pred = clf.predict(train_input)\n",
    "target_part = np.array(train_df[train_df.columns[-1:]].iloc[-432], dtype=np.float32).reshape(-1)\n",
    "pred_trans=[]\n",
    "first=True\n",
    "for diff in pred:\n",
    "    if first:\n",
    "        pred_trans.append(target_part[0]+diff)\n",
    "        first=False\n",
    "    else:\n",
    "        before = pred_trans[-1]\n",
    "        pred_trans.append(before+diff)\n",
    "real = np.array(train_df[train_df.columns[-1:]].iloc[-431:-144], dtype=np.float32).reshape(-1)\n",
    "print(\"Train Result Is ======================\")\n",
    "print(\"MSE ==\")\n",
    "print(mean_squared_error(real, pred_trans))\n",
    "print(\"MAE ==\")\n",
    "print(mean_absolute_error(real, pred_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 38)\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"./AIFrenz_Season1_dataset/test.csv\")\n",
    "sub_df.drop(['X14','X16','X19'], axis=1, inplace=True)\n",
    "print(sub_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaemin/opt/anaconda3/envs/py36t2/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/Users/jaemin/opt/anaconda3/envs/py36t2/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "feed_for_sub = np.array(sub_df[sub_df.columns[1:]])\n",
    "feed_for_sub = (feed_for_sub-global_mean)/global_std\n",
    "print(feed_for_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = gbm.predict(feed_for_sub, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Y18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4752</td>\n",
       "      <td>20.895868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4753</td>\n",
       "      <td>20.863322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4754</td>\n",
       "      <td>20.956668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4755</td>\n",
       "      <td>21.052310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4756</td>\n",
       "      <td>20.903975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        Y18\n",
       "0  4752  20.895868\n",
       "1  4753  20.863322\n",
       "2  4754  20.956668\n",
       "3  4755  21.052310\n",
       "4  4756  20.903975"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_idx_df = pd.read_csv(\"./AIFrenz_Season1_dataset/sample_submission.csv\")\n",
    "sub_idx_df[\"Y18\"] = sub_pred\n",
    "sub_idx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx_df.to_csv(\"./submit/sub_v05.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
